\documentclass[%
thesis=student,% bachlor's or master's thesis
coverpage=false,% do not print an extra cover page
titlepage=false,% do not print an extra title page
headmarks=true, % headmarks can be switched on or off
german,% or `english`
font=libertine, % use `libertine` font; alternatives: `helvet` / `palatino` / `times`
math=newpxtx, % math font `newpxtx`; alternatives: `ams`, `pxtx`
BCOR=5mm,% binding correction - adapt accordingly
coverBCOR=11mm% binding correction for the cover - adapt accordingly
]{tumbook}

%\makeatletter %redefine some labels from the TUM template
%\provideName{\@tum@examiner@}{Supervisor}{Themensteller} % or `Themenstellerin`
%\provideName{\@tum@supervisor@}{Advisors}{Betreuer} % or `Advisor` / `Betreuerin`
\makeatother

\usepackage{booktabs}% for more beautiful tables
\usepackage{cleveref}% intelligent references

\usepackage{diffcoeff}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheoremstyle{break}
{\topsep}   % Platz oberhalb
{\topsep}   % Platz unterhalb
{\itshape}  % Schriftart im Textkörper
{0pt}       % Einzug (keiner)
{\bfseries} % Schriftart im Kopf
{.}         % Punkt nach Theorem-Name
{\newline}  % Zeilenabstand nach Theorem-Name
{}          % Zusätzliche Spezifikation


\theoremstyle{break}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}[definition]{Satz}

%Literatur
\usepackage[%
    backend=bibtex, %, or `biber` on more up-to-date systems
    sortcites, % sort automatically
    sorting=nty, % sort order
    safeinputenc, % solves problems with unicode-formatted author names etc.
    citestyle=alphabetic, %
    bibstyle=alphabetic, %
    hyperref=true, % provide clickable links
    maxbibnames=3, % shorten author list for more than 3 names
    maxcitenames=3, % use at most 3 names for key
    url=false, % do not print URLs
    doi=false, % do not print DOIs
    giveninits=true,
    ]%
{biblatex}
\addbibresource{literature.bib}

% automatische Anführungszeichen
\usepackage[autostyle=true]{csquotes}


\title{Verzerrung der Inferenz bei der Verwendung gemischter Modelle in latentent Repräsentationen}


\author{Yannick Bantel}

\degree{Bachelor of Science}% or `Bachlor of Science`
\dateSubmitted{23. Juli 202}% preferably use some universally recognized date format

\examiner{Prof.\@ Dr.\@ Harald Binder}% `Themensteller`
\supervisor{Clemens Schächter}% `Betreuer`


\begin{document}

\frontmatter
\maketitle
\section*{Zusammenfassung}
Eine kurze Zusammenfassung der Arbeit auf Deutsch.

\section*{Abstract}
A brief abstract of this thesis in English.

\cleardoublepage{}

\tableofcontents

\mainmatter{}

%%---------------EINLEITUNG--------------------------------------------------
\chapter{Einleitung}

In der modernen Datenanalyse spielen gemischte Modelle eine zentrale Rolle, da sie es ermöglichen, sowohl feste als auch zufällige Effekte zu berücksichtigen, was sie besonders in den Bereichen der Biostatistik, der Sozialwissenschaften und der ökonomischen Modellierung populär macht. Im Rahmen dieser Arbeit werden gemischte Modelle auf die Analyse medizinischer Daten angewendet. Mit dem Aufkommen von Big Data und komplexen Datenstrukturen hat sich der Fokus zunehmend auf die effiziente und genaue Extraktion von Informationen aus großen und oft unübersichtlichen Datensätzen verlagert. In diesem Zusammenhang gewinnen latente Repräsentationen an Bedeutung, da sie es ermöglichen, inhärente Strukturen innerhalb der Daten zu identifizieren und zu nutzen, um tiefere Einblicke zu gewinnen.\\
\\
Die Integration von gemischten Modellen in latente Repräsentationen birgt jedoch das Risiko einer Verzerrung der Inferenzergebnisse, was die Genauigkeit und Zuverlässigkeit der aus den Daten gezogenen Schlussfolgerungen erheblich beeinträchtigen kann. Diese Arbeit befasst sich daher mit der Untersuchung der Verzerrungen, die bei der Anwendung gemischter Modelle auf latente Repräsentationen auftreten können. Ziel ist es, die Mechanismen zu verstehen, die zu diesen Verzerrungen führen, und Methoden zu entwickeln, um ihre Auswirkungen zu minimieren.\\
\\
Das Problem der Verzerrung ist besonders relevant, da eine fehlerhafte Inferenz zu Fehlentscheidungen führen kann, die in praktischen Anwendungen schwerwiegende Folgen haben können. Durch eine sorgfältige Analyse und Bewertung von gemischten Modellansätzen in Verbindung mit latenten Repräsentationen zielt diese Arbeit darauf ab, einen Beitrag zur Verbesserung der Modellgenauigkeit und der Zuverlässigkeit von Inferenzschlüssen zu leisten.\\
\\
Die Arbeit gliedert sich in mehrere Teile, die zunächst die theoretischen Grundlagen von gemischten Modellen und latenten Repräsentationen behandeln, gefolgt von einer Diskussion der Methoden zur Messung und Korrektur von Verzerrungen. Anhand von experimentellen Studien werden diese Konzepte dann praktisch angewendet und evaluiert, um abschließend Empfehlungen für die Anwendung dieser Techniken in Forschung und Praxis zu geben.\\
\\
\subsection{Motivation}


%%-------------THEORETISCHE-GRUNDLAGEN---------------------------------------
\chapter{Theoretische Grundlagen}
Definition, Typen und Anwendungsbereiche.\\
Bevor auf die Methodik dieser Arbeit eingegangen wird, ist eine theoretische Aufarbeitung der behandelten Themen notwendig. In diesem Kapitel werden die theoretischen Aspekte dieser Arbeit ausführlich beschrieben und behandelt. Sowohl lineare gemischte Modelle als auch die Theorie hinter variablen Autocodern werden eingeführt und beschrieben. Insbesondere wird in diesem Kapitel auf die für die Analyse der Modelle notwendige Theorie, wie z.B. die Likelihood-Berechnung und den Likelihood-Ratio-Test, eingegangen.
\section{Gemischte Modelle}
Um die theoretischen Aspekte der gemischten Modelle zu behandeln, folgen wir dem Buch \cite{pinheiro2000} Mixed-Effects Models in S and S-Plus von José C. Pinheiro und Douglas M. Bates \cite{pinheiro2000}.\\
\\
Ein gemischtes Modell ist ein statistisches Verfahren zur Datenanalyse, das sowohl feste als auch zufällige Effekte (fixed and random effects) modelliert. Gemischte Modelle werden vor allem bei der Analyse von Längsschnitt- und Clusterdaten eingesetzt.
Die parametrisierten festen und zufälligen Effekte berechnen zusammen mit einem Fehlervektor die Reaktionsvariable. Das lineare gemischte Modell für eine Gruppe ist wie folgt definiert
\begin{definition}[Lineares gemischtes Modell für Longitudinal- oder Clusterdaten] 
	Seien X ($n_i$ x p) und Z ($n_i$ x q) bekannte Designmatrizen für die festen und zufälligen Effekte. Seien $\beta$ ein p-dimensionaler Vektor von p festen Effekten und $b$ ein q-dimensionaler Vektor von q zufälligen Effekten und sei $\epsilon$ ein $n_i$-dimensionaler normalverteilter Fehlervektor.\\
	\\
	Ein lineares gemischtes Modell für den $n_i$-dimensionalen Antwortvektor der i-ten Gruppe wird durch 
	$$y_i = X_i * \beta + Z_i * b_i + \epsilon_i$$ 
	$$b_i \sim \mathcal{N}(0,\sum), \epsilon_i \sim \mathcal{N}(0,\sigma^2I)$$
	definiert.
\end{definition}\noindent
Die Daten der zufälligen und festen Effekte werden in einer Designmatrix (Datenmatrix) gespeichert.Die Parametervektoren $\beta$ (für die festen Effekte) und $b_i$ (für die zufälligen Effekte) initialisieren den Einfluss der Daten auf den Antwortvektor. Für immer auftretende Messfehler oder unerwartete Einflüsse wird ein zufälliges Rauschen $\epsilon$ hinzugefügt.\\
\\
Die zufälligen Effekte $b_i$ und der Fehlervektor sind in der gleichen Gruppe unabhängig voneinander und unabhängig von anderen Gruppen. Wie man leicht erkennen kann, ist der Vektor der zufälligen Effekte nur durch seine Varianz-Kovarianz-Matrix $\sum$ charakterisiert, die symmetrisch und positiv semidefinit ist.
\\
Die einzelnen Cluster/Gruppen können zu einem einzigen allgemeinen linearen gemischten Modell zusammengefasst werden:
\begin{definition}[Allgemeines lineares gemischtes Modell]
	Ein lineares gemischtes Modell ist definiert durch
	$$y = X\beta + Zb + \epsilon $$
	mit $$\begin{pmatrix}
		b \\
		\epsilon \\
	\end{pmatrix}
	\sim
	\mathcal{N}
	\begin{pmatrix}
		\begin{pmatrix}
			
			0 \\
			0 \\
		\end{pmatrix},
		\begin{pmatrix}
			\sum & 0 \\
			0 & \sigma^2I \\
		\end{pmatrix}
	\end{pmatrix}$$
	gegeben. Dabei sind $X$, bzw $Z$ die Designmatrizen der festen, bzw zufälligen Effekte, $\beta$ und $b$ die Parametervektoren der festen und der zufälligen Effekten und $\epsilon$ der Fehlervektor.
\end{definition}\noindent
Die Vektoren $b$ und $\epsilon$ sind also normalverteilt mit der Varianz $Var(b) = \sum$ und $Var(\epsilon) = R = \sigma^2I$. Die Varianz von $y$ kann daher geschrieben werden als $Var(y) = V = Z \sum Z^t + R.$
\section{Likelihood Inferenz}
Um den Einfluss eines fixen Effekts zu testen, vergleichen wir ein reduziertes Modell ohne diesen fixen Parameter mit dem vollständigen Modell. Dazu verwenden wir den Likelihood-Ratio-Test (LRT), der üblicherweise für diesen Zweck verwendet wird. Wie das Testverfahren genau funktioniert und wie der LRT durchgeführt wird, werden wir später sehen. Zuvor benötigen wir noch etwas Theorie zur Likelihood-Berechnung. 
\subsection{Likelihood Berechnung gemischter Modelle}
Dieser Abschnitt befasst sich mit der Schätzung der unbekannten Parameter. Wir folgen hier dem Ansatz von \cite{fahrmeir2010} .\\
\\
Wir verwenden die Maximum-Likelihood (ML)-Methode zur Berechnung der Schätzer. Eine Alternative wäre die restringierte ML-Methode, die jedoch nicht für den Likelihood-Ratio-Test geeignet ist. Daher verwenden wir für unsere Berechnung die ML-Methode.\\
\\
Die Schätzung der Parameter in einem gemischten Modell ist jedoch etwas komplizierter. Nicht nur $\beta$ ist unbekannt, sondern auch $b$, $\sum$ und $R$. Daher müssen sowohl die festen und zufälligen Effekte als auch die unbekannten Parameter, die wir als $\theta$ bezeichnen, in $\sum$ und $R$ geschätzt werden. Dies zwingt uns zu einer geschachtelten Schätzung.\\
\\
Nehmen wir zunächst an, dass $X,V$ und $Z$ bekannt sind.Für die Schätzung von $\beta$ aus dem Randmodell bietet sich der ML-Schätzer $$\hat{\beta} = (X^t V^{-1}X)^{-1}X^t V^{-1}y$$ an. Setzt man hier $\hat{V}$ ein, so erhält man den besten linearen erwartungstreuen Schätzer (BLUE, best linear unbiased estimator) für die fixen Effekte. \\
Für den Schätzer von $b$ verwenden wir den bedingten Erwartungswert $E(b|y)$ von $b$, gegeben die Daten $y$. \\
Betrachten wir die gemeinsame Verteilung von $b$ und $y$ 
$$\begin{pmatrix}
	y \\
	b \\
\end{pmatrix}
\sim
\mathbf{N}
\begin{pmatrix}
	\begin{pmatrix}
		
		X\beta \\
		0 \\
	\end{pmatrix},
	\begin{pmatrix}
		V & Z\sum \\
		\sum Z^t & \sum \\
	\end{pmatrix}
\end{pmatrix}$$
erhalten wir $E(b|y) = \sum Z^tV^{-1}(y-X\beta)$. \\
Ersetzt man nun $\beta$ durch den Schätzer $\hat{\beta}$ erhält man den Schätzer $$\hat{b} = \hat{\sum} Z^t\hat{V}^{-1}(y-X\hat{\beta})$$ für die zufälligen Effekte. Der Schätzer $\hat{b}$ ist der beste lineare unverzerrte Schätzer (BLUP, best linear unbiased prediction)\\

\begin{definition}[Schätzer für feste und zufällige Effekte]
	Sei $y = X\beta + Zb + \epsilon$ ein lineares gemischtes Modell und sei $V = Var(y)$.
	Dann ist $$\hat{\beta} = (X^t \hat{V}^{-1}X)^{-1}X^t \hat{V}^{-1}y$$ ein Schätzer für die festen Effekte und $$ \hat{b} = \hat{\tiny\sum} Z^t\hat{V}^{-1}(y-X\hat{\beta})$$ ein Schätzer für die zufälligen Effekte.
\end{definition}

Wie bereits erwähnt, soll $\theta$ der Parametervektor sein, der alle unbekannten Parameter in $V= V(\theta), \sum = \sum(\theta)$ und $R = R(\theta)$ enthält. Wenn wir nun einen Schätzer $\hat{\theta}$ haben, können wir durch Einsetzen von $\hat{\theta}$ die Kovarianzschätzer und damit die Schätzer der festen und zufälligen Effekte berechnen. 
Die ML-Methode für $\theta$ basiert auf dem Randmodell 
$$y \sim \mathbf{N}(X\beta,V(\theta)).$$
Die Log-Likelihood von $\beta$ und $\theta$ ist gegeben durch
$$l(\beta,\theta) = - \frac{1}{2} (log(|V|)+(y-X\beta)^tV^{-1}(y-X\beta)).$$
Maximieren von $l(\beta,\theta)$ bezüglich $\beta$ für festes $\theta$ ergibt 
$$ \hat{\beta} = (X^tV^{-1}X)^{-1}X^tV^{-1}y.$$
Setzt man nun $\hat{\theta}$ in $l(\beta,\theta)$ ein, so erhält man die Profil-Log-Wahrscheinlichkeit 
$$ l(\theta)_p = - \frac{1}{2} (log(|V|)+(y-X\hat{\beta})^tV^{-1}(y-X\hat{\beta})).$$
Folglich erhält man den ML-Schätzer $\hat{\theta}_{ML}$ durch Maximierung von $l(\theta)_p$. 
\begin{definition}[Kovarianz-Schätzer]
	Sei $y = X\beta + Zb + \epsilon $ ein lineares gemischtes Modell mit $$\begin{pmatrix}
		b \\
		\epsilon \\
	\end{pmatrix}
	\sim
	\mathcal{N}
	\begin{pmatrix}
		\begin{pmatrix}
			
			0 \\
			0 \\
		\end{pmatrix},
		\begin{pmatrix}
			\sum & 0 \\
			0 & R = \sigma^2I \\
		\end{pmatrix}
	\end{pmatrix}$$ und sei $\theta$ der unbekannte Parametervektor von $\sum$,$R$ und $V=Var(y)$. \\
	Dann ist $\hat{\theta}_{ML}$ der ML-Schätzer für $\theta$, den man durch maximieren von $$  l(\theta)_p = - \frac{1}{2} (log(|V|)+(y-X\hat{\beta})^tV^{-1}(y-X\hat{\beta})) $$erhält.
	Dabei ist $$ \hat{\beta} = (X^tV^{-1}X)^{-1}X^tV^{-1}y$$
\end{definition} \noindent
 Mit dem Schätzer $\hat{V}$ lassen sich die Schätzer der festen und zufälligen Effekte nun berechnen.
 

\begin{definition}[Maximum-Likelihood]
	Sei 
	$$ l(\sum,R) = -0.5 * (log(|V|)+ r'V^{-1}r + N * log(2\pi))$$
\end{definition}\noindent
\subsection{Likelihood-Ratio-Test}
Die Berechnung der Likelihood-Ratio-Test (LRT)-Statistik ist relativ einfach, wenn man sich die Theorie der ML-Methode vergegenwärtigt. Zur Erinnerung: Wir wollen ein reduziertes Modell mit dem vollständigen Modell vergleichen, um herauszufinden, wie groß der Einfluss einer Störgröße ist. Dazu verwenden wir den LRT. 
\begin{definition}[Likelihood-Ratio-Test (LRT)] 
	Sei $\mathbf{L}_{full}$ der Likelihood-Wert des vollständigen Modells und $\mathbf{L}_{red}$ der Likelihood-Wert des reduzierten Modells. Sei i die Anzahl der Freiheitsgrade. Dann ist die LRT Statistik gegeben durch 
	$$ LRT = 2(\mathbf{L}_{full}- \mathbf{L}_{red}) $$ 
\end{definition} \noindent
Falls $\mathbf{L}_{full}$ und $\mathbf{L}_{red}$ wie in der Definition initialisiert sind, gilt $\mathbf{L}_{full} > \mathbf{L}_{red}$. Insbesondere gilt $\log(\mathbf{L}_{full}) > \log(\mathbf{L}_{red})$.
Falls $\mathbf{L}_{full}$ und $\mathbf{L}_{red}$ nun schon die Log-Likelihood-Werte der Modelle sind, gilt für die Berechnung der LRT Statistik einfach $2(\mathbf{L}_{full} - \mathbf{L}_{red})$. \\
%%---------------VAE-------------------------------------------------------
\section{Variational Autoencoder}
Für die Anwendung der gemischten Modelle in einer latenten Repräsentation wird ein Variational Auto-Encoder (VAE) verwendet, der für die Methodik dieser Arbeit von Vorteil ist. 
Ein VAE ist ein künstliches neuronales Netzwerk, bestehend aus einem Encoder $q_\phi(z|x)$ und einem Decoder $p_\theta(x|z)$, das lernt, die wesentlichen Merkmale der Eingabedaten zu extrahieren und eine komprimierte Version dieser Daten zu erzeugen. Sie sind für latente Repräsentationen sehr interessant, da sie hochdimensionale Datensätze mit Hilfe des Encoders im latenten Raum niedrigdimensional darstellen können. Dies reduziert die Komplexität der Modellierung und ermöglicht es, gemischte Modelle effizienter und genauer zu betreiben. In einem niedrigdimensionalen Raum ist die Wahrscheinlichkeit geringer, dass die Modelle übermäßig komplexe oder verzerrte Strukturen lernen.\\ 
Im Vergleich zu einem üblichen Auto-Encoder, welcher aus dem Encoder im latenten Raum eine Vektordarstellung der Daten liefert, gibt der Encoder des VAE eine dimensionalreduzierte Darstellung der Daten als Wahrscheinlichkeitsverteilung aus. Der Encoder liefert also zwei Vektoren: den Mittelwert $\mu$ und die Standardabweichung $\sigma^2$. Durch diese verbesserte Darstellung ist der VAE im Gegensatz zu normalen Auto-Encodern in der Lage nicht nur den Input-Datensatz zu rekonstruieren, sondern auch andere ähnliche Datensätze.\\
\\
\subsection{Training VAE}
Ein VAE ist ein spezifisches Beispiel einer Variational Inference (VI). Ziel einer VI ist es die Posterior-Verteilung $p(z|x)$ der latenten Variablen gegeben der Input Daten zu berechnen. Diese Verteilung ist besonders bei komplexen Modellen schwer zu berechnen. Aufgrund dessen approximiert der VAE die Posterior-Verteilung durch eine einfachere Verteilung $q_\phi(x)$. Das Ziel von VIs ist es also die Kullback-Leibler-Divergenz zwischen $p(z|x)$ und $q_\phi(x)$ zu minimieren. 
\begin{definition}[Kullback-Leibler-Divergenz (KL-Divergenz)]
Sei $q_\phi(z|x)$ die approximierte Posterior-Gauß-Verteilung und $p(z)$ die Prior-Gauß-Verteilung. Dann ist die KL-Divergenz definiert als
$$D_{KL}(q_\phi(z|x)||p(z))= \int q_\phi(z|x) \log \left(\frac{q_\phi(z|x)}{p(z)} \right)dz$$
\end{definition}\noindent
Da nun $p(z)$ schwer zu berechnen ist optimieren wir stattdessen die Evidence Lower Bound (ELBO).
\begin{definition}[Evidence Lower Bound (ELBO)]
$$ \mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z) - \log q_\phi(z|x)] $$ 
\end{definition}\noindent
\begin{definition}[Jensensche Ungleichung]
	Sei g eine konvexe Funktion und  $x\in \mathcal{L^1}$. Dann gilt
	$$ \mathbb{E}[g(x)] \geq g(\mathbb{E}[x]) $$ 
\end{definition} \noindent
Also gilt $ \mathbb{E}[\log(x)] \geq \log(\mathbb{E}[x]) $.\\
\\
Das Objekt der Optimierung im VAE wird also der ELBO sein. \\
Wir betrachten nun die log-likelihood der Daten.\\
\begin{align}
 	\log p_\theta(x) &= \mathbf{E}_{q_\phi(z|x)}[\log p_\theta(x)] \\
 	&= \mathbb{E}_{q_\phi(z|x)}\left[\log\left[ \frac{p_\theta(x,z)}{p_\theta(z|x)}\right]\right] \\
 	&= \mathbb{E}_{q_\phi(z|x)}\left[\log\left[ \frac{p_\theta(x,z)q_\phi(z|x)}{q_\phi(z|x)p_\theta(z|x)}\right]\right] \\
 	&= \underbrace{\mathbb{E}_{q_\phi(z|x)}\left[\log\left[ \frac{p_\theta(x,z)}{q_\phi(z|x)}\right]\right]}_{\substack{\text{$= \mathcal{L}_{\theta,\phi}(x)$}\\\text{(ELBO)}}} + \underbrace{\mathbb{E}_{q_\phi(z|x)}\left[\log\left[ \frac{q_\phi(z|x)}{p_\theta(z|x)}\right]\right]}_{=D_{KL}(q_\phi(z|x)||p_\theta(z|x))}
\end{align}\noindent
Der zweite Term in der Gleichung ist die nicht-negative Kullback-Leibler-Divergenz (KL-Divergenz) zwischen$q_\phi(z|x)$ und $p_\theta(z|x))$. Die KL-Divergenz misst den Unterschied, bzw. die Ähnlichkeit, zweier Wahrscheinlichkeitsverteilungen.
Der erste Term in Gleichung (2.4) ist der variational lower bound, also der ELBO: 
$$\mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{q_\phi(z|x)}\left[\log p_\theta(x,z)- \log q_\phi(z|x)\right]$$
Wenn wir Gleichung (2.4) umstellen sehen wir, dass der ELBO eine untere Schranke für die log-likelihood der Daten ist, da die KL-Divergenz nicht negativ ist. 
\begin{align}
	\mathcal{L}_{\theta,\phi}(x) &= \log p_\theta(x) - D_{KL}(q_\phi(z|x)||p_\theta(z|x)) \\
	& \leq \log p_\theta(x)
\end{align}\noindent
Alternativ mit der Jensenschen Ungleichung:
\begin{align}
	\log p_\theta(x) &= \log \int p_\theta(x,z) dz \\
	&= \log \int p_\theta(x,z) \frac{q_\phi(z|x)}{q_\phi(z|x)} dz \\
	& = \log \mathbb{E}_{q_\phi(z|x)}\left[\frac{p_\theta(x,z)}{q_\phi(z|x)}\right] \\
	& \geq \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x,z)}{q_\phi(z|x)}\right] = \mathcal{L}_{\theta,\phi}(x)
\end{align}\noindent
Wenn also der ELBO bzgl. $\theta$ und  $\phi$ maximiert wird, wird die marginale likelihood $p_\theta(x)$ auch maximiert. Unser generatives Modell wird also besser. Gleichzeitig minimieren wir dadurch die KL-Divergenz der Approximation $q_\phi(z|x)$ von dem wahren Vorgänger $p_\theta(z|x)$. Demnach wird $q_\phi(z|x)$ optimiert. 
Die ELBO kann durch stochastische Gradientenverfahren optimiert werden.\\
\\
Wir wollen also den Gradienten von $\mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{q_\phi(z|x)}\left[\log p_\theta(x,z)- \log q_\phi(z|x)\right]$ bezüglich $\theta$ berechnen, was problematisch sein kann. Der übliche Monte Carlo Gradienten-Schätzer ist: 
$$ \nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)]= \mathbb{E}_{q_\phi(z)}[f(z)\nabla_{q_\phi(z)}\log q_\phi(z)] \approx \frac{1}{L} \sum_{l=1}^{L}f(z)\nabla_{q_\phi(z^{(l)})}\log q_\phi(z^{(l)})$$
wobei $z^{(l)} \sim q_\phi(z|x^{(i)})$ ist. 
Dies ist im Normalfall keine Herausforderung. Allerdings ist unsere Dichtefunktion nun auch abhängig von  $\theta$.  Um dieses Problem zu lösen nutzen wir den Reparameterization Trick.

Die Gradienten werden durch Monte-Carlo-Sampling geschätzt, wobei der Reparametrisierungstrick verwendet wird, um die Gradientenberechnung zu vereinfachen. Der Reparametrisierungstrick transformiert die Zufallsvariable  
\subsubsection{Reparametrisierungs Trick}
Der Reparameterisierungs-Trick ermöglicht die Berechnung der Gradienten der ELBO durch eine Transformation der Zufallsvariable zz in eine deterministische Funktion von $\epsilon$, einer Hilfsvariablen:
$$z=\mu + \sigma \odot \epsilon \hspace{10pt}\text{mit} \hspace{2pt} \epsilon\sim N(0,I).$$ 
Dadurch wird die ELBO als Funktion der Parameter $\theta$ und $\phi$ differenzierbar und kann mit stochastischen Gradientenverfahren optimiert werden.
\section{Verzerrung (Bias) und ihre Messung}
Erklärung von Verzerrung, wie sie entsteht und wie sie gemessen wird.

\chapter{Minimierung der Verzerrung}

%%--------------------METHODIK-----------------------------------------------
\chapter{Methodik}
\section{Vorgehen}
In den ersten Wochen habe ich mir selbst ein Simulationsdesign für einen longitudinalen medizinischen Datensatz ausgedacht und basierend darauf ein gemischtes Modell gefittet. Mit diesen simulierten Daten habe ich ein reduziertes Model mit dem vollständigen Model verglichen. Die LRT Statistik habe ich dann in einem Histogramm dargestellt.\\
\\
Wir fügen dem gemischten Modell einen festen Effekt hinzu, welcher keinen Einfluss auf die Trajektorie haben soll. In unserem Fall ist dieser feste Effekt das Geschlecht, welches keinen Einfluss auf den Verlauf einer Krankheit haben sollte.\\
\\
Mein zweites Projekt ist nun einen hoch dimensionalen medizinischen Datensatz durch den Encoder eines Variational Autoencoders im latenten Raum zu repräsentieren und dort mit einem gemischten Model darzustellen. Ähnlich wie zuvor will ich wieder eine LRT Statistik erhalten, in dem ich ein reduziertes Modell mit dem vollständigen Model vergleiche. Dazu trainiere ich in einer Schleife den Encoder und das gemischte Model für jeden Iterationsschritt neu und vergleiche die negativen Maximum Likelihood-Werte (ML-Werte) durch den Likelihood Ratio Test. Am Ende der Schleife erhalte ich wieder eine LRT Statistik, welche durch ein Histogramm dargestellt wird. Im Optimalfall ähnelt das Histogramm einer Chi-Quadrat-Verteilung mit einem Freiheitsgrad (Da das reduzierte Model nur einen festen Effekt, das Geschlecht, weniger hat).\\
\section{Datenbeschaffung}
Quellen und Typen der verwendeten Daten
\section{Modellierungstechniken}
Beschreibung der spezifischen gemischten Modelle und der Techniken zur Gewinnung latenter Repräsentationen.
\section{Analysemethoden}
Verfahren zur Untersuchung der Verzerrung in den Inferenzergebnissen.

%%------------EXPERIMENTE-UND-ERGEBNISSE-------------------------------------
\chapter{Experimente und Ergebnisse}
\section{Experimentelles Design}
Aufbau der experimentellen Tests und Simulationen.
\section{Durchführung}
Beschreibung der durchgeführten Experimente und verwendeten Parameter.
\section{Analyse der Ergebnisse}
Diskussion der Ergebnisse im Hinblick auf die Verzerrung der Inferenz.

%%------------DISKUSSION-----------------------------------------------------
\chapter{Diskussion}
\section{Interpretation der Ergebnisse}
Tiefere Analyse der Ergebnisse und ihrer Implikationen.
\section{Vergleich mit bestehenden Arbeiten}
Wie sich die Ergebnisse zu bereits veröffentlichten Forschungen verhalten.
\section{Limitationen und Herausforderungen}
Kritische Betrachtung der Grenzen der Studie und mögliche Probleme.
\chapter{Fazit}
Zusammenfassung der wichtigsten Erkenntnisse
Praktische Implikationen: Wie die Ergebnisse in der Praxis angewendet werden können.
Empfehlungen für zukünftige Forschungen: Vorschläge für weiterführende oder ergänzende Studien.
\chapter{Anhang}
\chapter{Literaturverzeichnis}
\appendix
\chapter{Appendix}
\section{Supporting Data}
\section{Some Code Listings}

\backmatter{}
\listoffigures% may be removed
\listoftables% may be removed

\nocite{Alspach:2008,GaleShapley:1962} % further literature that has not been explicitly referenced in the text
\printbibliography{} % print bibliography

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: default
%%% TeX-command-extra-options: "-shell-escape"
%%% ispell-local-dictionary: "american"
%%% eval: (setenv "TEXINPUTS" ".//:")
%%% TeX-master: t
%%% End:
