{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_models import Progress_Bar, Encoder, Decoder, CovarianceMatrix, thermometer_encode_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae latent dimension\n",
    "latent_dim = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_ind(id, df):\n",
    "    return np.where(df['patient_id'] == id)[0]\n",
    "\n",
    "def get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=1, include_interaction=False):\n",
    "    patient_id = df_effects['patient_id'].unique()\n",
    "\n",
    "    X_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), fixed_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "\n",
    "    if include_interaction==True:\n",
    "        for key in random_effects_keys[1:]:\n",
    "            X_list = [torch.cat((X_i, X_i[:,1:] * torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), key])).unsqueeze(-1)\n",
    "                                ), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "    X_list = [torch.cat((torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), 'age'])).unsqueeze(-1), X_i), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "\n",
    "    Z_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), random_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "    Z_list = [torch.block_diag(*[i for j in range(r)]) for i in Z_list]   \n",
    "    X_list = [torch.block_diag(*[i for j in range(r)]) for i in X_list]\n",
    "    return X_list, Z_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Encoder and Decoder Models and the Mixed Model Parameters. mode='diagonal': Diagonal Covariance Matrix, mode='full': Full Covariance Matrix,\n",
    "def initialize(latent_dim, mode='diagonal'):\n",
    "    encoder = Encoder(\n",
    "        input_dim=69,\n",
    "        hidden_dims=[150], \n",
    "        output_dim=latent_dim, \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    decoder = Decoder(\n",
    "        item_positions=np.concatenate([[i]*a for i,a in enumerate(np.array(test_scores_df[test_scores_df.columns[1:]].max(0)).astype(np.int32))]),                            \n",
    "        input_dim=latent_dim,\n",
    "        hidden_dims=[150], \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    var_param = CovarianceMatrix(q*latent_dim, mode=mode)    \n",
    "    return encoder, decoder, var_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Train the VAE model:  Weighting of the loss function\n",
    "# alpha: kl-divergence weight;  \n",
    "# delta: MSE distance between encoder prediction and decoder; \n",
    "# gamma: decoder reconstruction loss\n",
    "# eta: mixed model loss\n",
    "\n",
    "# batch_size should be greater than 50\n",
    "\n",
    "def train_vae(epochs, batch_size, encoder, decoder, optimizer_vae, alpha=1, gamma=1):\n",
    "    \n",
    "    steps = int(len(test_scores_df_encoded) / batch_size)\n",
    "    rng = np.random.default_rng(1234)\n",
    "    prior = Normal(torch.zeros(torch.Size([latent_dim])).to(device), torch.ones(torch.Size([latent_dim])).to(device))\n",
    "    #progBar = Progress_Bar(epochs, steps, ['nELBO', 'KL', 'Rec Loss', 'Item Error'])\n",
    "\n",
    "    encoder\n",
    "    decoder\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        shuffle = rng.permutation(len(test_scores_df_encoded))\n",
    "        \n",
    "        for step in range(steps):\n",
    "            pat_batch = np.arange(len(test_scores_df_encoded))[shuffle[step*batch_size:(step+1)*batch_size]]\n",
    "\n",
    "            test_data = torch.from_numpy(np.array(test_scores_df_encoded.loc[pat_batch])).to(torch.float32)\n",
    "            test_data_orig = torch.from_numpy(np.array(test_scores_df[test_scores_df.columns[1:]].loc[pat_batch])).to(torch.int32)\n",
    "            \n",
    "            optimizer_vae.zero_grad(set_to_none=True)\n",
    "            #encode test scores\n",
    "            mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "            #reparametrization trick to get latent variables\n",
    "            eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "            z = mu + log_sig.exp() * eps\n",
    "            \n",
    "            #kl divergence\n",
    "            kl = torch.mean(0.5 * torch.sum(mu.square() + torch.exp(2.0 * log_sig) - 1.0 - (2.0 * log_sig), dim=1))\n",
    "\n",
    "            rec_loss, probs = decoder(z, test_data_orig)\n",
    "            nelbo = alpha * kl + gamma * rec_loss\n",
    "                \n",
    "            nelbo.backward()\n",
    "            optimizer_vae.step()\n",
    "\n",
    "            #data_pred = torch.stack([torch.argmax(pred, dim=-1) for pred in probs]) \n",
    "            # total test item prediction error  \n",
    "            #item_error = np.mean(np.sum(np.abs(data_pred.detach().numpy() - test_data_orig.T.numpy()), axis=0))\n",
    "\n",
    "            # progBar.update({\n",
    "            #     'nELBO': nelbo.item(), \n",
    "            #     'KL': alpha * kl.item(), \n",
    "            #     'Rec Loss': gamma * rec_loss.item(), \n",
    "            #     'Item Error': item_error,\n",
    "            #     }) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mu, log_sig= encoder.encode(torch.from_numpy(np.array(test_scores_df_encoded)).to(torch.float32))\n",
    "        epsilon = prior.sample(torch.Size([log_sig.size(dim=0)]))\n",
    "        z = mu + log_sig.exp() * epsilon\n",
    "        return z.detach().cpu()\n",
    "\n",
    "\n",
    "def calc_likelihood(var_param, Z_list, X_list, z_list):\n",
    "    Phi, sigma = var_param()\n",
    "\n",
    "    N = sum([len(Z_i) for Z_i in Z_list])\n",
    "\n",
    "    V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0),device=device) * sigma for Z_i in Z_list]\n",
    "    V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "    \n",
    "    Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list, V_inv_list)]).sum(dim=0)\n",
    "    Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "    #Check if Xt_V_inv_X is invertible. Only needed for mini batching\n",
    "    if torch.abs(torch.det(Xt_V_inv_X)) > 1e-6:\n",
    "        EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "        #EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list_batch, Z_list_batch, V_inv_list, z_list)]\n",
    "\n",
    "        residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list)]\n",
    "        #Mixed model prediction\n",
    "        #z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list_batch, Z_list_batch, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "        log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "        const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "        rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "        #negative mixed models likelihood\n",
    "        nML = 0.5 * (log_det_V + rt_V_inv_r + N * const) #/ N   \n",
    "         \n",
    "    return nML \n",
    "\n",
    "def train_vae_2(epochs, batch_size, encoder, decoder, optimizer_vae, Z_list, X_list, var_param, alpha=1, gamma=1, eta=0.01):\n",
    "    steps = int(num_patients / batch_size)\n",
    "    rng = np.random.default_rng(1234)\n",
    "    prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        shuffle = rng.permutation(num_patients)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            #draw minibatch\n",
    "            pat_batch = patients[shuffle[step*batch_size:(step+1)*batch_size]]\n",
    "            pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in pat_batch]\n",
    "            \n",
    "            ind_batch = []\n",
    "            add = 0\n",
    "            for ind in range(len(pat_ind_batch)):\n",
    "                len_i = len(pat_ind_batch[ind])\n",
    "                ind_batch += [torch.arange(add, add + len_i)]\n",
    "                add += len_i\n",
    "\n",
    "            Z_list_batch = [Z_list[pat] for pat in pat_batch]\n",
    "            X_list_batch = [X_list[pat] for pat in pat_batch]\n",
    "            \n",
    "            test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "            test_data_orig = torch.concatenate([torch.from_numpy(np.array(test_scores_df[test_scores_df.columns[1:]].loc[ind])).to(torch.int32) for ind in pat_ind_batch])\n",
    "            \n",
    "            optimizer_vae.zero_grad(set_to_none=True)\n",
    "            #encode test scores\n",
    "            mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "            #reparametrization trick to get latent variables\n",
    "            eps = prior.sample(torch.Size([log_sig.size(dim=0)]))\n",
    "            z = mu + log_sig.exp() * eps\n",
    "            \n",
    "            #kl divergence\n",
    "            kl = torch.mean(0.5 * torch.sum(mu.square() + torch.exp(2.0 * log_sig) - 1.0 - (2.0 * log_sig), dim=1))\n",
    "\n",
    "            # get the response variable list (latent z)\n",
    "            z_list = [z[ind].flatten().to(torch.float32) for ind in ind_batch]\n",
    "\n",
    "            #Mixed model loglikelihood loss. Notation follows https://www.sfu.ca/sasdoc/sashtml/stat/chap41/sect23.htm\n",
    "            Phi, sigma = var_param()\n",
    "\n",
    "            V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list_batch]\n",
    "            V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "            \n",
    "            Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list_batch, V_inv_list)]).sum(dim=0)\n",
    "            Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list_batch, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "            #Check if Xt_V_inv_X is invertible. Only needed for mini batching\n",
    "            if torch.abs(torch.det(Xt_V_inv_X)) > 1e-6:\n",
    "                EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "                EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list_batch, Z_list_batch, V_inv_list, z_list)]\n",
    "        \n",
    "                #Mixed model prediction\n",
    "                z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list_batch, Z_list_batch, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "\n",
    "                #distance between encoder latent variables and mixed model prediction\n",
    "                residuals = ((z_pred - z) ** 2).sum(1).mean()\n",
    "\n",
    "                #reconstruction loss\n",
    "                rec_loss, probs = decoder(z_pred, test_data_orig)\n",
    "                nelbo = alpha * kl + eta * residuals + gamma * rec_loss \n",
    "                \n",
    "                nelbo.backward()\n",
    "                optimizer_vae.step()    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mu, log_sig = encoder.encode(torch.from_numpy(np.array(test_scores_df_encoded)).to(torch.float32))\n",
    "        epsilon = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "        z = mu + log_sig.exp() * epsilon\n",
    "        return z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(L_full, L_red):\n",
    "    return 2 * (L_full - L_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\n",
      " 1.Trainingsschleife\n",
      "\n",
      " 2.Trainingsschleife\n",
      "\n",
      " Iteration: 0\n",
      "\n",
      " Iteration: 1\n",
      "\n",
      " Iteration: 2\n",
      "\n",
      " Iteration: 3\n",
      "\n",
      " Iteration: 4\n",
      "\n",
      " Iteration: 5\n",
      "\n",
      " Iteration: 6\n",
      "\n",
      " Iteration: 7\n",
      "\n",
      " Iteration: 8\n",
      "\n",
      " Iteration: 9\n",
      "\n",
      " Iteration: 10\n",
      "\n",
      " Iteration: 11\n",
      "\n",
      " Iteration: 12\n",
      "\n",
      " Iteration: 13\n",
      "\n",
      " Iteration: 14\n",
      "\n",
      " Iteration: 15\n",
      "\n",
      " Iteration: 16\n",
      "\n",
      " Iteration: 17\n",
      "\n",
      " Iteration: 18\n",
      "\n",
      " Iteration: 19\n",
      "\n",
      " Iteration: 20\n",
      "\n",
      " Iteration: 21\n",
      "\n",
      " Iteration: 22\n",
      "\n",
      " Iteration: 23\n",
      "\n",
      " Iteration: 24\n",
      "\n",
      " Iteration: 25\n",
      "\n",
      " Iteration: 26\n",
      "\n",
      " Iteration: 27\n",
      "\n",
      " Iteration: 28\n",
      "\n",
      " Iteration: 29\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.inv: The diagonal element 2 is zero, the inversion could not be completed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m     res\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m--> 104\u001b[0m \u001b[43moptimizer_mm_red\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m res_red \u001b[38;5;241m=\u001b[39m calc_likelihood(var_param_red, Z_list_red, X_list_red, z_list)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# res_full = minimize(calc_likelihood_full, var_param_full.parameters(), method='bfgs', max_iter=6)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# res_red = minimize(calc_likelihood_red, var_param_red.parameters(), method='bfgs', max_iter=6)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m#     res_red.backward()\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#     optimizer_mm_red.step()  \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\lbfgs.py:440\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 440\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    441\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    442\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 100\u001b[0m, in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[0;32m     99\u001b[0m     optimizer_mm_red\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 100\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_param_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_list_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_list_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     res\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "Cell \u001b[1;32mIn[9], line 72\u001b[0m, in \u001b[0;36mcalc_likelihood\u001b[1;34m(var_param, Z_list_batch, X_list_batch, z_list)\u001b[0m\n\u001b[0;32m     69\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(Z_i) \u001b[38;5;28;01mfor\u001b[39;00m Z_i \u001b[38;5;129;01min\u001b[39;00m Z_list_batch])\n\u001b[0;32m     71\u001b[0m V_list \u001b[38;5;241m=\u001b[39m [Z_i \u001b[38;5;241m@\u001b[39m Phi \u001b[38;5;241m@\u001b[39m Z_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(Z_i\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m sigma \u001b[38;5;28;01mfor\u001b[39;00m Z_i \u001b[38;5;129;01min\u001b[39;00m Z_list_batch]\n\u001b[1;32m---> 72\u001b[0m V_inv_list \u001b[38;5;241m=\u001b[39m [V_i\u001b[38;5;241m.\u001b[39minverse() \u001b[38;5;28;01mfor\u001b[39;00m V_i \u001b[38;5;129;01min\u001b[39;00m V_list]\n\u001b[0;32m     74\u001b[0m Xt_V_inv_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([X_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m V_i_inv \u001b[38;5;241m@\u001b[39m X_i \u001b[38;5;28;01mfor\u001b[39;00m X_i, V_i_inv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_list_batch, V_inv_list)])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     75\u001b[0m Xt_V_inv_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([X_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m V_i_inv \u001b[38;5;241m@\u001b[39m y_i \u001b[38;5;28;01mfor\u001b[39;00m X_i, V_i_inv, y_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_list_batch, V_inv_list, z_list)])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(Z_i) \u001b[38;5;28;01mfor\u001b[39;00m Z_i \u001b[38;5;129;01min\u001b[39;00m Z_list_batch])\n\u001b[0;32m     71\u001b[0m V_list \u001b[38;5;241m=\u001b[39m [Z_i \u001b[38;5;241m@\u001b[39m Phi \u001b[38;5;241m@\u001b[39m Z_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(Z_i\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m sigma \u001b[38;5;28;01mfor\u001b[39;00m Z_i \u001b[38;5;129;01min\u001b[39;00m Z_list_batch]\n\u001b[1;32m---> 72\u001b[0m V_inv_list \u001b[38;5;241m=\u001b[39m [\u001b[43mV_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m V_i \u001b[38;5;129;01min\u001b[39;00m V_list]\n\u001b[0;32m     74\u001b[0m Xt_V_inv_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([X_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m V_i_inv \u001b[38;5;241m@\u001b[39m X_i \u001b[38;5;28;01mfor\u001b[39;00m X_i, V_i_inv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_list_batch, V_inv_list)])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     75\u001b[0m Xt_V_inv_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([X_i\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m V_i_inv \u001b[38;5;241m@\u001b[39m y_i \u001b[38;5;28;01mfor\u001b[39;00m X_i, V_i_inv, y_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_list_batch, V_inv_list, z_list)])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31m_LinAlgError\u001b[0m: linalg.inv: The diagonal element 2 is zero, the inversion could not be completed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "num_simulations = 40\n",
    "iterations = 30\n",
    "lrt_results = []\n",
    "\n",
    "fixed_effects_keys_full = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys_full = ['intercept', 'since_medication', 'since_switch']\n",
    "# reduced model without fixed effect 'sex' \n",
    "fixed_effects_keys_red = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never']\n",
    "random_effects_keys_red = ['intercept', 'since_medication', 'since_switch']\n",
    "q = len(random_effects_keys_full)\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    print(f\"Epoch: {i}\")\n",
    "    #prepare dataset\n",
    "    test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "    test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "    time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "    time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "    baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "    baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "    df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "\n",
    "    patients = torch.from_numpy(np.array(baseline_df['patient_id']))\n",
    "    num_patients = len(patients)\n",
    "\n",
    "    X_list_full, Z_list_full = get_design_matrix(df_effects, fixed_effects_keys_full, random_effects_keys_full, r=latent_dim)\n",
    "    X_list_red, Z_list_red = get_design_matrix(df_effects, fixed_effects_keys_red, random_effects_keys_red, r=latent_dim)   \n",
    "    \n",
    "    pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list_full])\n",
    "\n",
    "    encoder, decoder = initialize(latent_dim)[0:2]\n",
    "    optimizer_vae = torch.optim.Adam([ \n",
    "        {'params': encoder.parameters(), 'lr': 0.01},  \n",
    "        {'params': decoder.parameters(), 'lr': 0.01},  \n",
    "    ])\n",
    "\n",
    "    var_param_full = CovarianceMatrix(q*latent_dim, mode='diagonal')\n",
    "    var_param_red = CovarianceMatrix(q*latent_dim, mode='diagonal')   \n",
    "\n",
    "    optimizer_mm_full = torch.optim.LBFGS([ \n",
    "        {'params': var_param_full.parameters(), 'lr': 0.5}\n",
    "    ])\n",
    "    optimizer_mm_red = torch.optim.LBFGS([ \n",
    "        {'params': var_param_red.parameters(), 'lr': 0.5}\n",
    "    ])\n",
    "    # optimizer_mm_full = optim.LBFGS(var_param_full.parameters(), lr=1.0)\n",
    "    # optimizer_mm_red = optim.LBFGS(var_param_red.parameters(), lr=1.0)\n",
    "    print(\"\\n 1.Trainingsschleife\")\n",
    "    z = train_vae(2, 128, encoder, decoder, optimizer_vae)\n",
    "    pat_ind_b = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "    z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_b]\n",
    "    def closure():\n",
    "        optimizer_mm_full.zero_grad()\n",
    "        res = calc_likelihood(var_param_full, Z_list_full, X_list_full, z_list)\n",
    "        res.backward()\n",
    "        return res\n",
    "        \n",
    "    optimizer_mm_full.step(closure)\n",
    "\n",
    "    print(\"\\n 2.Trainingsschleife\")\n",
    "    for j in range(iterations):\n",
    "        optimizer_mm_full = torch.optim.LBFGS([ \n",
    "            {'params': var_param_full.parameters(), 'lr': 0.5} # 0.25 falss not a number\n",
    "        ])\n",
    "        print(f\"\\n Iteration: {j}\")\n",
    "        z = train_vae_2(1, 128, encoder, decoder, optimizer_vae, Z_list_full, X_list_full, var_param_full, alpha=1, gamma=1, eta=10)\n",
    "        z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_b]\n",
    "\n",
    "        def closure():\n",
    "            optimizer_mm_full.zero_grad()\n",
    "            res = calc_likelihood(var_param_full, Z_list_full, X_list_full, z_list)\n",
    "            res.backward()\n",
    "            return res\n",
    "        \n",
    "        optimizer_mm_full.step(closure) \n",
    "        #ohne batches\n",
    "        #res = minimize(calc_likelihood_full, var_param_full.parameters(), method='bfgs', max_iter=6)\n",
    "\n",
    "        # steps = 300\n",
    "        # for k in range(steps):\n",
    "        #     optimizer_mm_full.zero_grad(set_to_none=True)\n",
    "        #     nML_full = calc_likelihood_full(var_param_full)\n",
    "        #     nML_full.backward()\n",
    "        #     optimizer_mm_full.step(closure) \n",
    "        #z_pred = calc_likelihood_full_model(var_param_full, Z_list_full, X_list_full, z_list)\n",
    "    \n",
    "\n",
    "\n",
    "    # def closure():\n",
    "    #     optimizer_mm_full.zero_grad()\n",
    "    #     res = calc_likelihood_full(var_param_full)\n",
    "    #     res.backward()\n",
    "    #     return res\n",
    "        \n",
    "    # optimizer_mm_full.step(closure)\n",
    "    res_full = calc_likelihood(var_param_full, Z_list_full, X_list_full, z_list)\n",
    "\n",
    "    def closure():\n",
    "        optimizer_mm_red.zero_grad()\n",
    "        res = calc_likelihood(var_param_red, Z_list_red, X_list_red, z_list)\n",
    "        res.backward()\n",
    "        return res\n",
    "        \n",
    "    optimizer_mm_red.step(closure)\n",
    "    res_red = calc_likelihood(var_param_red, Z_list_red, X_list_red, z_list)\n",
    "\n",
    "    # res_full = minimize(calc_likelihood_full, var_param_full.parameters(), method='bfgs', max_iter=6)\n",
    "    # res_red = minimize(calc_likelihood_red, var_param_red.parameters(), method='bfgs', max_iter=6)\n",
    "\n",
    "    # steps = 300\n",
    "    # print('\\nTrain full:')\n",
    "    # for j in range(steps):\n",
    "    #     optimizer_mm_full.zero_grad(set_to_none=True)\n",
    "    #     res_full = calc_likelihood_full(var_param_full)\n",
    "    #     res_full.backward()\n",
    "    #     optimizer_mm_full.step()\n",
    "\n",
    "    # print('\\nTrain reduced:')\n",
    "    # for k in range(steps):\n",
    "    #     optimizer_mm_red.zero_grad(set_to_none=True)\n",
    "    #     res_red = calc_likelihood_red(var_param_red)\n",
    "    #     res_red.backward()\n",
    "    #     optimizer_mm_red.step()  \n",
    "\n",
    "    lrt_results.append(likelihood_ratio(res_full, res_red))\n",
    "\n",
    "\n",
    "# mit os kann datei in gleichen ordner gespeichert wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\n",
      " 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 72\u001b[0m\n\u001b[0;32m     64\u001b[0m optimizer_mm_full \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([ \n\u001b[0;32m     65\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: var_param_full\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}\n\u001b[0;32m     66\u001b[0m ])\n\u001b[0;32m     68\u001b[0m optimizer_mm_red \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([ \n\u001b[0;32m     69\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: var_param_red\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}\n\u001b[0;32m     70\u001b[0m ])\n\u001b[1;32m---> 72\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_vae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m pat_ind_b \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39marange(pat_ind[i],pat_ind[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m patients]\n\u001b[0;32m     75\u001b[0m z_list \u001b[38;5;241m=\u001b[39m [z[ind]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m pat_ind_b]\n",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m, in \u001b[0;36mtrain_vae\u001b[1;34m(epochs, batch_size, encoder, decoder, optimizer_vae, alpha, gamma)\u001b[0m\n\u001b[0;32m     40\u001b[0m         nelbo \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m kl \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m rec_loss\n\u001b[0;32m     42\u001b[0m         nelbo\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 43\u001b[0m         \u001b[43moptimizer_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m#data_pred = torch.stack([torch.argmax(pred, dim=-1) for pred in probs]) \u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# total test item prediction error  \u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;66;03m#item_error = np.mean(np.sum(np.abs(data_pred.detach().numpy() - test_data_orig.T.numpy()), axis=0))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m#     'Item Error': item_error,\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m#     }) \u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN SIMULATION FOR n = num_simulations\n",
    "num_simulations = 1000\n",
    "lrt_results = [] # List to save the results\n",
    "\n",
    "all_epochs_dict = {} #dictionary to save all models in only one frame\n",
    "\n",
    "fixed_effects_keys_full = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys_full = ['intercept', 'since_medication', 'since_switch']\n",
    "# reduced model without fixed effect 'sex' \n",
    "fixed_effects_keys_red = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never']\n",
    "random_effects_keys_red = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "q = len(random_effects_keys_full)\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    print('epoch', i)\n",
    "    #Load dataframes: test_scores has high dimensional test score data of hfsme tests\n",
    "    test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "\n",
    "    #test_scores_df_encoded is a thermometer encoding of test_scores_df for the encoder network\n",
    "    test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "\n",
    "    #time_df contains data that changes with time, e.g.: age or time since medication switch\n",
    "    time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "    time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "\n",
    "    #baseline_df contains features that characterizes patients at baseline\n",
    "    baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "\n",
    "    # 'sex' has no influence:\n",
    "    baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "\n",
    "    df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "    \n",
    "    patients = torch.from_numpy(np.array(baseline_df['patient_id'])).to(device)\n",
    "    num_patients = len(patients)\n",
    "    \n",
    "    # get design matrix for the full model\n",
    "    X_list_full, Z_list_full = get_design_matrix(df_effects, fixed_effects_keys_full, random_effects_keys_full, r=latent_dim)\n",
    "    X_list_full = [X.to(device) for X in X_list_full]\n",
    "    Z_list_full = [Z.to(device) for Z in Z_list_full]\n",
    "    # get design matrix for the reduced model\n",
    "    X_list_red, Z_list_red = get_design_matrix(df_effects, fixed_effects_keys_red, random_effects_keys_red, r=latent_dim)  \n",
    "    X_list_red = [X.to(device) for X in X_list_red]\n",
    "    Z_list_red = [Z.to(device) for Z in Z_list_red]\n",
    "    \n",
    "    pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list_full])\n",
    "    \n",
    "    \n",
    "\n",
    "    print('\\n', i)\n",
    "    # reinitialize the parameters and the optimizer\n",
    "    encoder, decoder = initialize(latent_dim)[0:2]\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    optimizer_vae = torch.optim.Adam([ \n",
    "        {'params': encoder.parameters(), 'lr': 0.01},  \n",
    "        {'params': decoder.parameters(), 'lr': 0.01},  \n",
    "    ])\n",
    "    \n",
    "    var_param_full = CovarianceMatrix(q*latent_dim, mode='diagonal').to(device) \n",
    "    var_param_red = CovarianceMatrix(q*latent_dim, mode='diagonal').to(device)\n",
    "    \n",
    "    optimizer_mm_full = torch.optim.Adam([ \n",
    "        {'params': var_param_full.parameters(), 'lr': 1.0}\n",
    "    ])\n",
    "\n",
    "    optimizer_mm_red = torch.optim.Adam([ \n",
    "        {'params': var_param_red.parameters(), 'lr': 1.0}\n",
    "    ])\n",
    "    \n",
    "    z = train_vae(150, 100, encoder, decoder, optimizer_vae)\n",
    "\n",
    "    pat_ind_b = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "    z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_b]\n",
    "    \n",
    "    steps = 300\n",
    "    #progBar_full = Progress_Bar(steps, 1, ['nML'])\n",
    "    #progBar_red = Progress_Bar(steps, 1, ['nML'])\n",
    "\n",
    "    print('\\nTrain full:')\n",
    "    for j in range(steps):\n",
    "        optimizer_mm_full.zero_grad(set_to_none=True)\n",
    "        nML_full = calc_likelihood(var_param_full, Z_list_full, X_list_full, z_list)\n",
    "        nML_full.backward()\n",
    "        optimizer_mm_full.step()       \n",
    "        # progBar_full.update({\n",
    "        #     'nML': nML_full.item(), \n",
    "        #     }) \n",
    "\n",
    "    print('\\nTrain reduced:')\n",
    "    for k in range(steps):\n",
    "        optimizer_mm_red.zero_grad(set_to_none=True)\n",
    "        nML_red = calc_likelihood(var_param_red, Z_list_red, X_list_red, z_list)\n",
    "        nML_red.backward()\n",
    "        optimizer_mm_red.step()            \n",
    "        # progBar_red.update({\n",
    "        #     'nML': nML_red.item(), \n",
    "        #     }) \n",
    "\n",
    "    all_epochs_dict[f'epoch_{i}'] = {\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'var_param_full': var_param_full,\n",
    "        'optimizer_vae_state_dict': optimizer_vae.state_dict(),\n",
    "        'var_param_red': var_param_red,\n",
    "        'optimizer_mm_full': optimizer_mm_full,\n",
    "        'optimizer_mm_red': optimizer_mm_red,\n",
    "        'X_list_full' : X_list_full,\n",
    "        'Z_list_full' : Z_list_full,\n",
    "        'X_list_red' : X_list_red,\n",
    "        'Z_list_red' : Z_list_red,\n",
    "        'z_list': z_list\n",
    "    }\n",
    "\n",
    "    print('\\nLikelihood ratio:', likelihood_ratio(nML_full, nML_red))\n",
    "    lrt_results.append(likelihood_ratio(nML_full, nML_red))\n",
    "\n",
    "torch.save(all_epochs_dict, fr'C:\\Users\\yanni\\OneDrive\\Desktop\\BachelorArbeit2024\\Code\\trained_models\\vae_MM_getrennt_{num_simulations}_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the histogr<am of the LRT-statistic\n",
    "plt.hist(np.array(torch.stack(lrt_results).detach()), bins=50, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# x = np.linspace(-8000, 8000, 100)\n",
    "# plt.plot(x, chi2.pdf(x + 500, df=1), 'r-', lw=2, label='Chi-Quadrat-Verteilung (df=1)')\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "plt.plot(-x, chi2.pdf(x, df=1), 'r-', lw=1, label='Chi-Squared (df=1)')\n",
    "\n",
    "# Beschriftungen hinzufügen\n",
    "plt.title('Histogramm der Likelihood-Ratio-Teststatistiken')\n",
    "plt.xlabel('Teststatistik')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.legend()\n",
    "\n",
    "# Histogramm anzeigen\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
