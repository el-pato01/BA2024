{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from get_models import Progress_Bar, Encoder, Decoder, CovarianceMatrix, thermometer_encode_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torchmin import minimize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframes: test_scores has high dimensional test score data of hfsme tests\n",
    "test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "\n",
    "#test_scores_df_encoded is a thermometer encoding of test_scores_df for the encoder network\n",
    "test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "\n",
    "#time_df contains data tht changes with time, e.g.: age or time since medication switch\n",
    "time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "\n",
    "#baseline_df contains features that characterizes patients at baseline\n",
    "baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "\n",
    "# 'sex' has no influence:\n",
    "baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "\n",
    "df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "\n",
    "fixed_effects_keys = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "p = len(fixed_effects_keys)\n",
    "q = len(random_effects_keys)\n",
    "\n",
    "#vae latent dimension\n",
    "latent_dim = 2\n",
    "\n",
    "def get_ind(id, df):\n",
    "    return np.where(df['patient_id'] == id)[0]\n",
    "\n",
    "def get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=1, include_interaction=False):\n",
    "    patient_id = df_effects['patient_id'].unique()\n",
    "\n",
    "    X_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), fixed_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "\n",
    "    if include_interaction==True:\n",
    "        for key in random_effects_keys[1:]:\n",
    "            X_list = [torch.cat((X_i, X_i[:,1:] * torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), key])).unsqueeze(-1)\n",
    "                                ), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "    X_list = [torch.cat((torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), 'age'])).unsqueeze(-1), X_i), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "\n",
    "    Z_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), random_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "    Z_list = [torch.block_diag(*[i for j in range(r)]) for i in Z_list]   \n",
    "    X_list = [torch.block_diag(*[i for j in range(r)]) for i in X_list]\n",
    "    return X_list, Z_list\n",
    "\n",
    "#get the design matrices\n",
    "X_list, Z_list = get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=latent_dim, include_interaction=False)\n",
    "\n",
    "pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Encoder and Decoder Models and the Mixed Model Parameters. mode='diagonal': Diagonal Covariance Matrix, mode='full': Full Covariance Matrix,\n",
    "def initialize(latent_dim, mode='diagonal'):\n",
    "    encoder = Encoder(\n",
    "        input_dim=np.shape(test_scores_df_encoded)[-1],\n",
    "        hidden_dims=[150], \n",
    "        output_dim=latent_dim, \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    decoder = Decoder(\n",
    "        item_positions=np.concatenate([[i]*a for i,a in enumerate(np.array(test_scores_df[test_scores_df.columns[1:]].max(0)).astype(np.int32))]),                            \n",
    "        input_dim=latent_dim,\n",
    "        hidden_dims=[150], \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    var_param = CovarianceMatrix(q*latent_dim, mode=mode)    \n",
    "    return encoder, decoder, var_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = torch.from_numpy(np.array(baseline_df['patient_id']))\n",
    "num_patients = len(patients)\n",
    "\n",
    "#Train the VAE model:  Weighting of the loss function\n",
    "# alpha: kl-divergence weight;  \n",
    "# delta: MSE distance between encoder prediction and decoder; \n",
    "# gamma: decoder reconstruction loss\n",
    "# eta: mixed model loss\n",
    "\n",
    "# batch_size should be greater than 50\n",
    "\n",
    "def train_vae(epochs,batch_size, X_list, Z_list, encoder, decoder, var_param, optimizer_vae, alpha=1, gamma=1, delta=1, eta=1):\n",
    "    steps = int(num_patients / batch_size)\n",
    "    rng = np.random.default_rng(1234)\n",
    "    prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "    progBar = Progress_Bar(epochs, steps, ['nELBO', 'KL', 'nML', 'Rec Loss', 'Residuals', 'Item Error'])\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        shuffle = rng.permutation(num_patients)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            #draw minibatch\n",
    "            pat_batch = patients[shuffle[step*batch_size:(step+1)*batch_size]]\n",
    "            pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in pat_batch]\n",
    "            \n",
    "            ind_batch = []\n",
    "            add = 0\n",
    "            for ind in range(len(pat_ind_batch)):\n",
    "                len_i = len(pat_ind_batch[ind])\n",
    "                ind_batch += [torch.arange(add, add + len_i)]\n",
    "                add += len_i\n",
    "\n",
    "            Z_list_batch = [Z_list[pat] for pat in pat_batch]\n",
    "            X_list_batch = [X_list[pat] for pat in pat_batch]\n",
    "\n",
    "            test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "            test_data_orig = torch.concatenate([torch.from_numpy(np.array(test_scores_df[test_scores_df.columns[1:]].loc[ind])).to(torch.int32) for ind in pat_ind_batch])\n",
    "            \n",
    "            optimizer_vae.zero_grad(set_to_none=True)\n",
    "            #encode test scores\n",
    "            mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "            #reparametrization trick to get latent variables\n",
    "            eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "            z = mu + log_sig.exp() * eps\n",
    "            \n",
    "            #kl divergence\n",
    "            kl = torch.mean(0.5 * torch.sum(mu.square() + torch.exp(2.0 * log_sig) - 1.0 - (2.0 * log_sig), dim=1))\n",
    "\n",
    "            # get the response variable list (latent z)\n",
    "            z_list = [z[ind].flatten().to(torch.float32) for ind in ind_batch]\n",
    "\n",
    "            #Mixed model loglikelihood loss. Notation follows https://www.sfu.ca/sasdoc/sashtml/stat/chap41/sect23.htm\n",
    "            Phi, sigma = var_param()\n",
    "            N = sum([len(Z_i) for Z_i in Z_list_batch])\n",
    "\n",
    "            V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list_batch]\n",
    "            V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "            \n",
    "            Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list_batch, V_inv_list)]).sum(dim=0)\n",
    "            Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list_batch, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "            #Check if Xt_V_inv_X is invertible. Only needed for mini batching\n",
    "            if torch.abs(torch.det(Xt_V_inv_X)) > 1e-6:\n",
    "                EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "                EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list_batch, Z_list_batch, V_inv_list, z_list)]\n",
    "\n",
    "                residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list_batch)]\n",
    "                #Mixed model prediction\n",
    "                z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list_batch, Z_list_batch, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "                log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "                const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "                rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "                #negative mixed models likelihood\n",
    "                nML = 0.5 * (log_det_V + rt_V_inv_r + N * const) / N\n",
    "                \n",
    "                #distance between encoder latent variables and mixed model prediction\n",
    "                residuals = ((z_pred - z) ** 2).sum(1).mean()\n",
    "\n",
    "                #reconstruction loss\n",
    "                rec_loss, probs = decoder(z_pred, test_data_orig)\n",
    "                nelbo = alpha * kl + delta * residuals + gamma * rec_loss + eta * nML\n",
    "                \n",
    "                nelbo.backward()\n",
    "                optimizer_vae.step()\n",
    "\n",
    "                data_pred = torch.stack([torch.argmax(pred, dim=-1) for pred in probs]) \n",
    "                # total test item prediction error  \n",
    "                item_error = np.mean(np.sum(np.abs(data_pred.detach().numpy() - test_data_orig.T.numpy()), axis=0))\n",
    "\n",
    "                progBar.update({\n",
    "                    'nELBO': nelbo.item(), \n",
    "                    'KL': alpha * kl.item(), \n",
    "                    'nML': eta * nML.item(),\n",
    "                    'Rec Loss': gamma * rec_loss.item(), \n",
    "                    'Residuals': delta * residuals.item(),\n",
    "                    'Item Error': item_error,\n",
    "                    }) \n",
    "            \n",
    "\n",
    "encoder, decoder, var_param = initialize(latent_dim, mode='diagonal')\n",
    "optimizer_vae = torch.optim.Adam([\n",
    "    {'params': var_param.parameters(), 'lr': 0.1},  \n",
    "    {'params': encoder.parameters(), 'lr': 0.01},  \n",
    "    {'params': decoder.parameters(), 'lr': 0.01},  \n",
    "])  \n",
    "\n",
    "#train_vae(300, 60, alpha=1, gamma=1, delta=1, eta=1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the model prediction:\n",
    "def eval_vae(X_list, Z_list, var_param, encoder):\n",
    "    with torch.no_grad():\n",
    "        pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "        prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "\n",
    "        test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "\n",
    "        mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "        eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "        z = mu + log_sig.exp() * eps\n",
    "\n",
    "        z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_batch]\n",
    "\n",
    "        Phi, sigma = var_param()\n",
    "        N = sum([len(Z_i) for Z_i in Z_list])\n",
    "\n",
    "        V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list]\n",
    "        V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "        \n",
    "        Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list, V_inv_list)]).sum(dim=0)\n",
    "        Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "        EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "        EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list, Z_list, V_inv_list, z_list)]\n",
    "\n",
    "        residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list)]\n",
    "        z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list, Z_list, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "        log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "        const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "        rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "        nML = 0.5 * (log_det_V + rt_V_inv_r + N * const)\n",
    "        return mu, z, z_pred, nML\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(L_full, L_red):\n",
    "    return 2 * (L_full - L_red)\n",
    "\n",
    "# returns a block diagonal matrix of a list of matrices\n",
    "def block_diag_list(arrs_list):\n",
    "    shape = torch.sum(torch.tensor([a.shape for a in arrs_list], dtype=torch.int32), dim=0)\n",
    "    dtype = arrs_list[0].dtype\n",
    "    device = arrs_list[0].device\n",
    "\n",
    "    out = torch.zeros(*shape.tolist(), dtype=dtype, device=device)\n",
    "\n",
    "    r, c = 0, 0\n",
    "    for a in arrs_list:\n",
    "        rows, cols = a.shape\n",
    "        out[r:r + rows, c:c + cols] = a\n",
    "        r += rows\n",
    "        c += cols\n",
    "    return out\n",
    "\n",
    "def calc_likelihood(var_param, Z_list_batch, X_list_batch, z_list):\n",
    "    Phi, sigma = var_param()\n",
    "    N = sum([len(Z_i) for Z_i in Z_list_batch])\n",
    "\n",
    "    V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list_batch]\n",
    "    V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "    \n",
    "    Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list_batch, V_inv_list)]).sum(dim=0)\n",
    "    Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list_batch, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "    #Check if Xt_V_inv_X is invertible. Only needed for mini batching\n",
    "    if torch.abs(torch.det(Xt_V_inv_X)) > 1e-6:\n",
    "        EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "        #EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list_batch, Z_list_batch, V_inv_list, z_list)]\n",
    "\n",
    "        residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list_batch)]\n",
    "        #Mixed model prediction\n",
    "        #z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list_batch, Z_list_batch, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "        log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "        const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "        rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "        #negative mixed models likelihood\n",
    "        nML = 0.5 * (log_det_V + rt_V_inv_r + N * const) #/ N   \n",
    "         \n",
    "    return nML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SIMULATION FOR n = num_simulations\n",
    "num_simulations = 10\n",
    "lrt_results= [] # List to save the results\n",
    "\n",
    "all_epochs_dict = {} #dictionary to save all models in only one file\n",
    "\n",
    "fixed_effects_keys_full = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys_full = ['intercept', 'since_medication', 'since_switch']\n",
    "# reduced model without fixed effect 'sex' \n",
    "fixed_effects_keys_red = ['family_affected', 'sco_surg', '≤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never']\n",
    "random_effects_keys_red = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    print('epoch', i)\n",
    "    #Load dataframes: test_scores has high dimensional test score data of hfsme tests\n",
    "    test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "\n",
    "    #test_scores_df_encoded is a thermometer encoding of test_scores_df for the encoder network\n",
    "    test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "\n",
    "    #time_df contains data that changes with time, e.g.: age or time since medication switch\n",
    "    time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "    time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "\n",
    "    #baseline_df contains features that characterizes patients at baseline\n",
    "    baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "\n",
    "    # 'sex' has no influence:\n",
    "    baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "\n",
    "    df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "    \n",
    "    patients = torch.from_numpy(np.array(baseline_df['patient_id']))\n",
    "    num_patients = len(patients)\n",
    "    \n",
    "    # get design matrix for the full model\n",
    "    X_list_full, Z_list_full = get_design_matrix(df_effects, fixed_effects_keys_full, random_effects_keys_full, r=latent_dim)\n",
    "    # get design matrix for the reduced model\n",
    "    X_list_red, Z_list_red = get_design_matrix(df_effects, fixed_effects_keys_red, random_effects_keys_red, r=latent_dim)   \n",
    "    \n",
    "    pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list_full])\n",
    "    # reinitialize the parameters and the optimizer\n",
    "    encoder_full, decoder_full, var_param_full = initialize(latent_dim) \n",
    "    optimizer_vae_full = torch.optim.Adam([\n",
    "        {'params': var_param_full.parameters(), 'lr': 0.1},  \n",
    "        {'params': encoder_full.parameters(), 'lr': 0.01},  \n",
    "        {'params': decoder_full.parameters(), 'lr': 0.01},  \n",
    "    ])\n",
    "    train_vae(200, 60, X_list_full, Z_list_full, encoder_full, decoder_full, var_param_full, optimizer_vae_full)\n",
    "    z  = eval_vae(X_list_full, Z_list_full, var_param_full, encoder_full)[1]\n",
    "    nML_full = eval_vae(X_list_full, Z_list_full, var_param_full, encoder_full)[3] # get the nML value \n",
    "    \n",
    "    var_param_red = CovarianceMatrix(q*latent_dim, mode='diagonal')  \n",
    "        \n",
    "    optimizer_mm_red = torch.optim.Adam([ \n",
    "        {'params': var_param_red.parameters(), 'lr': 1.0}\n",
    "    ])\n",
    "\n",
    "    pat_ind_b = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "    z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_b]\n",
    "\n",
    "    steps = 300\n",
    "    print('\\nTrain reduced:')\n",
    "    for k in range(steps):\n",
    "        optimizer_mm_red.zero_grad(set_to_none=True)\n",
    "        nML_red = calc_likelihood(var_param_red, Z_list_red, X_list_red, z_list)\n",
    "        nML_red.backward()\n",
    "        optimizer_mm_red.step()  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
