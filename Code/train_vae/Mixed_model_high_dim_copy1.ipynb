{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from get_models import Progress_Bar, Encoder, Decoder, CovarianceMatrix, thermometer_encode_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torchmin import minimize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframes: test_scores has high dimensional test score data of hfsme tests\n",
    "test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "\n",
    "#test_scores_df_encoded is a thermometer encoding of test_scores_df for the encoder network\n",
    "test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "\n",
    "#time_df contains data tht changes with time, e.g.: age or time since medication switch\n",
    "time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "\n",
    "#baseline_df contains features that characterizes patients at baseline\n",
    "baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "\n",
    "# 'sex' has no influence:\n",
    "baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "\n",
    "df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "\n",
    "fixed_effects_keys = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "p = len(fixed_effects_keys)\n",
    "q = len(random_effects_keys)\n",
    "\n",
    "#vae latent dimension\n",
    "latent_dim = 2\n",
    "\n",
    "def get_ind(id, df):\n",
    "    return np.where(df['patient_id'] == id)[0]\n",
    "\n",
    "def get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=1, include_interaction=False):\n",
    "    patient_id = df_effects['patient_id'].unique()\n",
    "\n",
    "    X_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), fixed_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "\n",
    "    if include_interaction==True:\n",
    "        for key in random_effects_keys[1:]:\n",
    "            X_list = [torch.cat((X_i, X_i[:,1:] * torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), key])).unsqueeze(-1)\n",
    "                                ), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "    X_list = [torch.cat((torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), 'age'])).unsqueeze(-1), X_i), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "\n",
    "    Z_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), random_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "    Z_list = [torch.block_diag(*[i for j in range(r)]) for i in Z_list]   \n",
    "    X_list = [torch.block_diag(*[i for j in range(r)]) for i in X_list]\n",
    "    return X_list, Z_list\n",
    "\n",
    "#get the design matrices\n",
    "X_list, Z_list = get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=latent_dim, include_interaction=False)\n",
    "\n",
    "pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Encoder and Decoder Models and the Mixed Model Parameters. mode='diagonal': Diagonal Covariance Matrix, mode='full': Full Covariance Matrix,\n",
    "def initialize(latent_dim, mode='diagonal'):\n",
    "    encoder = Encoder(\n",
    "        input_dim=np.shape(test_scores_df_encoded)[-1],\n",
    "        hidden_dims=[150], \n",
    "        output_dim=latent_dim, \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    decoder = Decoder(\n",
    "        item_positions=np.concatenate([[i]*a for i,a in enumerate(np.array(test_scores_df[test_scores_df.columns[1:]].max(0)).astype(np.int32))]),                            \n",
    "        input_dim=latent_dim,\n",
    "        hidden_dims=[150], \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    var_param = CovarianceMatrix(q*latent_dim, mode=mode)    \n",
    "    return encoder, decoder, var_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = torch.from_numpy(np.array(baseline_df['patient_id']))\n",
    "num_patients = len(patients)\n",
    "\n",
    "#Train the VAE model:  Weighting of the loss function\n",
    "# alpha: kl-divergence weight;  \n",
    "# delta: MSE distance between encoder prediction and decoder; \n",
    "# gamma: decoder reconstruction loss\n",
    "# eta: mixed model loss\n",
    "\n",
    "# batch_size should be greater than 50\n",
    "\n",
    "def train_vae(epochs,batch_size, X_list, Z_list, encoder, decoder, var_param, optimizer_vae, alpha=1, gamma=1, delta=1, eta=1):\n",
    "    steps = int(num_patients / batch_size)\n",
    "    rng = np.random.default_rng(1234)\n",
    "    prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "    progBar = Progress_Bar(epochs, steps, ['nELBO', 'KL', 'nML', 'Rec Loss', 'Residuals', 'Item Error'])\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        shuffle = rng.permutation(num_patients)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            #draw minibatch\n",
    "            pat_batch = patients[shuffle[step*batch_size:(step+1)*batch_size]]\n",
    "            pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in pat_batch]\n",
    "            \n",
    "            ind_batch = []\n",
    "            add = 0\n",
    "            for ind in range(len(pat_ind_batch)):\n",
    "                len_i = len(pat_ind_batch[ind])\n",
    "                ind_batch += [torch.arange(add, add + len_i)]\n",
    "                add += len_i\n",
    "\n",
    "            Z_list_batch = [Z_list[pat] for pat in pat_batch]\n",
    "            X_list_batch = [X_list[pat] for pat in pat_batch]\n",
    "\n",
    "            test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "            test_data_orig = torch.concatenate([torch.from_numpy(np.array(test_scores_df[test_scores_df.columns[1:]].loc[ind])).to(torch.int32) for ind in pat_ind_batch])\n",
    "            \n",
    "            optimizer_vae.zero_grad(set_to_none=True)\n",
    "            #encode test scores\n",
    "            mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "            #reparametrization trick to get latent variables\n",
    "            eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "            z = mu + log_sig.exp() * eps\n",
    "            \n",
    "            #kl divergence\n",
    "            kl = torch.mean(0.5 * torch.sum(mu.square() + torch.exp(2.0 * log_sig) - 1.0 - (2.0 * log_sig), dim=1))\n",
    "\n",
    "            # get the response variable list (latent z)\n",
    "            z_list = [z[ind].flatten().to(torch.float32) for ind in ind_batch]\n",
    "\n",
    "            #Mixed model loglikelihood loss. Notation follows https://www.sfu.ca/sasdoc/sashtml/stat/chap41/sect23.htm\n",
    "            Phi, sigma = var_param()\n",
    "            N = sum([len(Z_i) for Z_i in Z_list_batch])\n",
    "\n",
    "            V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list_batch]\n",
    "            V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "            \n",
    "            Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list_batch, V_inv_list)]).sum(dim=0)\n",
    "            Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list_batch, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "            #Check if Xt_V_inv_X is invertible. Only needed for mini batching\n",
    "            if torch.abs(torch.det(Xt_V_inv_X)) > 1e-6:\n",
    "                EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "                EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list_batch, Z_list_batch, V_inv_list, z_list)]\n",
    "\n",
    "                residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list_batch)]\n",
    "                #Mixed model prediction\n",
    "                z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list_batch, Z_list_batch, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "                log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "                const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "                rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "                #negative mixed models likelihood\n",
    "                nML = 0.5 * (log_det_V + rt_V_inv_r + N * const) / N\n",
    "                \n",
    "                #distance between encoder latent variables and mixed model prediction\n",
    "                residuals = ((z_pred - z) ** 2).sum(1).mean()\n",
    "\n",
    "                #reconstruction loss\n",
    "                rec_loss, probs = decoder(z_pred, test_data_orig)\n",
    "                nelbo = alpha * kl + delta * residuals + gamma * rec_loss + eta * nML\n",
    "                \n",
    "                nelbo.backward()\n",
    "                optimizer_vae.step()\n",
    "\n",
    "                data_pred = torch.stack([torch.argmax(pred, dim=-1) for pred in probs]) \n",
    "                # total test item prediction error  \n",
    "                item_error = np.mean(np.sum(np.abs(data_pred.detach().numpy() - test_data_orig.T.numpy()), axis=0))\n",
    "\n",
    "                progBar.update({\n",
    "                    'nELBO': nelbo.item(), \n",
    "                    'KL': alpha * kl.item(), \n",
    "                    'nML': eta * nML.item(),\n",
    "                    'Rec Loss': gamma * rec_loss.item(), \n",
    "                    'Residuals': delta * residuals.item(),\n",
    "                    'Item Error': item_error,\n",
    "                    }) \n",
    "            \n",
    "\n",
    "encoder, decoder, var_param = initialize(latent_dim, mode='diagonal')\n",
    "optimizer_vae = torch.optim.Adam([\n",
    "    {'params': var_param.parameters(), 'lr': 0.1},  \n",
    "    {'params': encoder.parameters(), 'lr': 0.01},  \n",
    "    {'params': decoder.parameters(), 'lr': 0.01},  \n",
    "])  \n",
    "\n",
    "#train_vae(300, 60, alpha=1, gamma=1, delta=1, eta=1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the model prediction:\n",
    "def eval_vae(X_list, Z_list, var_param, encoder):\n",
    "    with torch.no_grad():\n",
    "        pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "        prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "\n",
    "        test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "\n",
    "        mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "        eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "        z = mu + log_sig.exp() * eps\n",
    "\n",
    "        z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_batch]\n",
    "\n",
    "        Phi, sigma = var_param()\n",
    "        N = sum([len(Z_i) for Z_i in Z_list])\n",
    "\n",
    "        V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list]\n",
    "        V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "        \n",
    "        Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list, V_inv_list)]).sum(dim=0)\n",
    "        Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "        EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "        EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list, Z_list, V_inv_list, z_list)]\n",
    "\n",
    "        residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list)]\n",
    "        z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list, Z_list, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "        log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "        const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "        rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "        nML = 0.5 * (log_det_V + rt_V_inv_r + N * const)\n",
    "        return mu, z, z_pred, nML\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(L_full, L_red):\n",
    "    return 2 * (L_full - L_red)\n",
    "\n",
    "# returns a block diagonal matrix of a list of matrices\n",
    "def block_diag_list(arrs_list):\n",
    "    shape = torch.sum(torch.tensor([a.shape for a in arrs_list], dtype=torch.int32), dim=0)\n",
    "    dtype = arrs_list[0].dtype\n",
    "    device = arrs_list[0].device\n",
    "\n",
    "    out = torch.zeros(*shape.tolist(), dtype=dtype, device=device)\n",
    "\n",
    "    r, c = 0, 0\n",
    "    for a in arrs_list:\n",
    "        rows, cols = a.shape\n",
    "        out[r:r + rows, c:c + cols] = a\n",
    "        r += rows\n",
    "        c += cols\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mProgress\u001b[00m: 76.8%\u001b[96m - \u001b[00m\u001b[95mETA\u001b[00m: 0:05:14\u001b[96m - \u001b[00m\u001b[95mEpoch\u001b[00m: 154\u001b[96m - \u001b[00m\u001b[95mIteration\u001b[00m: 614\u001b[96m - \u001b[00m\u001b[95mms/Iteration\u001b[00m: 1693\u001b[96m - \u001b[00m\u001b[33mnELBO\u001b[00m: 8.6185 (\u001b[92m+0.559\u001b[00m)\u001b[96m - \u001b[00m\u001b[33mKL\u001b[00m: 1.3826 (\u001b[92m+0.123\u001b[00m)\u001b[96m - \u001b[00m\u001b[33mnML\u001b[00m: 0.0000 (---)\u001b[96m - \u001b[00m\u001b[33mRec Loss\u001b[00m: 7.2359 (\u001b[92m+0.435\u001b[00m)\u001b[96m - \u001b[00m\u001b[33mResiduals\u001b[00m: 0.0000 (---)\u001b[96m - \u001b[00m\u001b[33mItem Error\u001b[00m: 3.8961 (\u001b[92m+0.213\u001b[00m).           "
     ]
    }
   ],
   "source": [
    "latent_dim =  1\n",
    "X_list, Z_list = get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=latent_dim, include_interaction=False)\n",
    "pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list])\n",
    "encoder, decoder, var_param = initialize(latent_dim, mode='diagonal')\n",
    "optimizer_vae = torch.optim.Adam([\n",
    "    #{'params': var_param.parameters(), 'lr': 0.1},  \n",
    "    {'params': encoder.parameters(), 'lr': 0.01},  \n",
    "    {'params': decoder.parameters(), 'lr': 0.01},  \n",
    "]) \n",
    "train_vae(200,60, X_list, Z_list, encoder, decoder, var_param, optimizer_vae, delta=0, eta=0)\n",
    "latent_rep = eval_vae(X_list, Z_list, var_param, encoder)[1].clone().detach().float()\n",
    "latent_data = latent_rep.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 10\n",
    "\n",
    "fixed_effects_keys_full = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys_full = ['intercept', 'since_medication', 'since_switch']\n",
    "# reduced model without fixed effect 'sex' \n",
    "fixed_effects_keys_red = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never']\n",
    "random_effects_keys_red = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "#X_list_full, Z_list_full, y_list =get_design_matrix_y(df__fixed_effects, fixed_effects_keys_full,df_random_effects, random_effects_keys_full,df_response_variable, response_keys, r=1)\n",
    "# get design matrix for the reduced model \n",
    "#X_list_red, Z_list_red = get_design_matrix_y(df_fixed_effects, fixed_effects_keys_red, df_random_effects, random_effects_keys_red, df_response_variable, response_keys, r=1)\n",
    "\n",
    "X_list_full, Z_list_full = get_design_matrix(df_effects, fixed_effects_keys_full, random_effects_keys_full, r=latent_dim, include_interaction=False)\n",
    "X_list_red, Z_list_red = get_design_matrix(df_effects, fixed_effects_keys_red, random_effects_keys_red, r=latent_dim, include_interaction=False)\n",
    "y = latent_data\n",
    "\n",
    "n_fixed_eff = len(fixed_effects_keys) + 1\n",
    "n_rand_eff = len(random_effects_keys) \n",
    "n_fixed_eff_r = len(fixed_effects_keys_red) + 1\n",
    "n_rand_eff_r = len(random_effects_keys_red) \n",
    "softplus = torch.nn.Softplus()\n",
    "lrt_results = []\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    D_param = torch.randn(n_rand_eff, requires_grad=True) # Learnable parameter for diagonal covariance matrix\n",
    "    D_param_r = torch.randn(n_rand_eff_r, requires_grad=True) # Learnable parameter for diagonal covariance matrix\n",
    "    N = len(latent_data)\n",
    "    y = torch.from_numpy(np.array(latent_data)).to(torch.float32) # Response variable: Test scores\n",
    "    X_tilde_list = [torch.cat((X_i, torch.zeros((n_rand_eff, n_fixed_eff)))) for X_i in X_list_full] # Defining list of X tilde (eq. 2.7 p. 63)\n",
    "    y_e = torch.cat([torch.cat((y[get_ind(pat,df_effects)], torch.zeros(n_rand_eff))).unsqueeze(-1) for pat in df_effects['patient_id'].unique()]) # Augmented response vector y_e (eq. 2.11 p. 65)\n",
    "\n",
    "    def calculate_likelihood_full(D_param): \n",
    "        # Ensure that the diagonal covariance matrix has only positive values\n",
    "        Delta = torch.diag(softplus(D_param))\n",
    "        det_Delta = torch.det(Delta)  \n",
    "\n",
    "        # Defining list of Z tilde (eq. 2.7 p. 63)\n",
    "        Z_tilde_list = [torch.cat((Z_i, Delta)) for Z_i in Z_list_full]\n",
    "        # Matrix X_e (eq. 2.11 p. 65)\n",
    "        X_e = torch.cat((block_diag_list(Z_tilde_list), torch.cat(X_tilde_list)), -1)\n",
    "        # Calculate MLE estimates of random and fixed effects with current set of covariance Parameters (eq. 2.11 p. 65)\n",
    "        pred = torch.inverse((X_e.t() @ X_e)) @ X_e.t() @ y_e\n",
    "        # Calculate MLE estimates of the noise sigma with current set of covariance Parameters (eq. 2.12 p. 65)\n",
    "        pred_sigma = torch.sum((y_e - X_e @ pred) ** 2) / N\n",
    "\n",
    "        # Calculate the logarithm of the likelihood function in (eq. 2.13 p. 65)\n",
    "        likelihood = -N/2 * (1 + torch.log(torch.tensor(2 * torch.pi)) + torch.log(pred_sigma))\n",
    "        likelihood += torch.log(torch.stack([det_Delta/torch.det(Z_i_tilde.t() @ Z_i_tilde).sqrt() for Z_i_tilde in Z_tilde_list])).sum()\n",
    "        return - likelihood\n",
    "    \n",
    "    result_full = minimize(calculate_likelihood_full, D_param, method='bfgs', max_iter=6)\n",
    "    Lmax = result_full.fun\n",
    "\n",
    "    N_r = len(latent_data)\n",
    "    y_r = torch.from_numpy(np.array(latent_data)).to(torch.float32) # Response variable: Test scores\n",
    "    X_tilde_list_r = [torch.cat((X_i, torch.zeros((n_rand_eff_r, n_fixed_eff_r)))) for X_i in X_list_red] # Defining list of X tilde (eq. 2.7 p. 63)\n",
    "    y_e_r = torch.cat([torch.cat((y[get_ind(pat,df_effects)], torch.zeros(n_rand_eff))).unsqueeze(-1) for pat in df_effects['patient_id'].unique()])\n",
    "\n",
    "    def calculate_likelihood_reduced(D_param_r):\n",
    "        # Ensure that the diagonal covariance matrix has only positive values\n",
    "        Delta_r = torch.diag(softplus(D_param_r))\n",
    "        det_Delta_r = torch.det(Delta_r)  \n",
    "\n",
    "        # Defining list of Z tilde (eq. 2.7 p. 63)\n",
    "        Z_tilde_list_r = [torch.cat((Z_i, Delta_r)) for Z_i in Z_list_red]\n",
    "        # Matrix X_e (eq. 2.11 p. 65)\n",
    "        X_e_r = torch.cat((block_diag_list(Z_tilde_list_r), torch.cat(X_tilde_list_r)), -1)\n",
    "\n",
    "        # Calculate MLE estimates of random and fixed effects with current set of covariance Parameters (eq. 2.11 p. 65)\n",
    "        pred_r = torch.inverse((X_e_r.t() @ X_e_r)) @ X_e_r.t() @ y_e_r\n",
    "        # Calculate MLE estimates of the noise sigma with current set of covariance Parameters (eq. 2.12 p. 65)\n",
    "        pred_sigma_r = torch.sum((y_e_r - X_e_r @ pred_r) ** 2) / N_r\n",
    "\n",
    "        # Calculate the logarithm of the likelihood function in (eq. 2.13 p. 65)\n",
    "        likelihood = -N_r/2 * (1 + torch.log(torch.tensor(2 * torch.pi)) + torch.log(pred_sigma_r))\n",
    "        likelihood += torch.log(torch.stack([det_Delta_r/torch.det(Z_i_tilde_r.t() @ Z_i_tilde_r).sqrt() for Z_i_tilde_r in Z_tilde_list_r])).sum()\n",
    "        return - likelihood\n",
    "    result_reduced = minimize(calculate_likelihood_reduced, D_param_r, method='bfgs', max_iter=6)\n",
    "    Lmin = result_reduced.fun\n",
    "\n",
    "    lrt_results.append(likelihood_ratio(Lmin,Lmax))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
