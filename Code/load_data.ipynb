{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder:\n\tsize mismatch for mu.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([2, 150]).\n\tsize mismatch for mu.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for log_sig.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([2, 150]).\n\tsize mismatch for log_sig.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 155\u001b[0m\n\u001b[0;32m    152\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myanni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBachelorArbeit2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrained_models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Verwendung der geladenen Daten\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[43mencoder_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_full_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m decoder_full\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_full_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    157\u001b[0m var_param_full \u001b[38;5;241m=\u001b[39m loaded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar_param_full\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encoder:\n\tsize mismatch for mu.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([2, 150]).\n\tsize mismatch for mu.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for log_sig.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([2, 150]).\n\tsize mismatch for log_sig.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "from get_models import Progress_Bar, Encoder, Decoder, CovarianceMatrix, thermometer_encode_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2\n",
    "\n",
    "#Load dataframes: test_scores has high dimensional test score data of hfsme tests\n",
    "test_scores_df = pd.read_csv(os.getcwd()+'/test_scores.csv')\n",
    "\n",
    "#test_scores_df_encoded is a thermometer encoding of test_scores_df for the encoder network\n",
    "test_scores_df_encoded = thermometer_encode_df(test_scores_df, test_scores_df.columns[1:])\n",
    "\n",
    "#time_df contains data that changes with time, e.g.: age or time since medication switch\n",
    "time_df = pd.read_csv(os.getcwd()+'/time_df.csv')\n",
    "time_df['intercept'] = np.ones(time_df.shape[0])\n",
    "\n",
    "#baseline_df contains features that characterizes patients at baseline\n",
    "baseline_df = pd.read_csv(os.getcwd()+'/baseline_df.csv')\n",
    "\n",
    "# 'sex' has no influence:\n",
    "baseline_df['sex'] = np.random.randint(2, size=baseline_df.shape[0])\n",
    "\n",
    "df_effects = pd.merge(baseline_df, time_df, on='patient_id', how='inner')\n",
    "\n",
    "fixed_effects_keys = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "p = len(fixed_effects_keys)\n",
    "q = len(random_effects_keys)\n",
    "\n",
    "#vae latent dimension\n",
    "latent_dim = 2\n",
    "\n",
    "def get_ind(id, df):\n",
    "    return np.where(df['patient_id'] == id)[0]\n",
    "\n",
    "def get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=1, include_interaction=False):\n",
    "    patient_id = df_effects['patient_id'].unique()\n",
    "\n",
    "    X_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), fixed_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "\n",
    "    if include_interaction==True:\n",
    "        for key in random_effects_keys[1:]:\n",
    "            X_list = [torch.cat((X_i, X_i[:,1:] * torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), key])).unsqueeze(-1)\n",
    "                                ), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "    X_list = [torch.cat((torch.from_numpy(np.array(df_effects.loc[get_ind(patient_id[j], df_effects), 'age'])).unsqueeze(-1), X_i), -1).to(torch.float32) for j,X_i in enumerate(X_list)]\n",
    "\n",
    "\n",
    "    Z_list = [torch.from_numpy(np.array(df_effects.loc[get_ind(id, df_effects), random_effects_keys])).to(torch.float32) for id in patient_id]\n",
    "    Z_list = [torch.block_diag(*[i for j in range(r)]) for i in Z_list]   \n",
    "    X_list = [torch.block_diag(*[i for j in range(r)]) for i in X_list]\n",
    "    return X_list, Z_list\n",
    "\n",
    "#get the design matrices\n",
    "X_list, Z_list = get_design_matrix(df_effects, fixed_effects_keys, random_effects_keys, r=latent_dim, include_interaction=False)\n",
    "\n",
    "pat_ind = np.cumsum([0]+[int(len(X_i)/latent_dim) for X_i in X_list])\n",
    "# initialize Encoder and Decoder Models and the Mixed Model Parameters. mode='diagonal': Diagonal Covariance Matrix, mode='full': Full Covariance Matrix,\n",
    "def initialize(latent_dim, mode='diagonal'):\n",
    "    encoder = Encoder(\n",
    "        input_dim=np.shape(test_scores_df_encoded)[-1],\n",
    "        hidden_dims=[150], \n",
    "        output_dim=latent_dim, \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    decoder = Decoder(\n",
    "        item_positions=np.concatenate([[i]*a for i,a in enumerate(np.array(test_scores_df[test_scores_df.columns[1:]].max(0)).astype(np.int32))]),                            \n",
    "        input_dim=latent_dim,\n",
    "        hidden_dims=[150], \n",
    "        act=torch.nn.Tanh())\n",
    "\n",
    "    var_param = CovarianceMatrix(q*latent_dim, mode=mode)    \n",
    "    return encoder, decoder, var_param\n",
    "patients = torch.from_numpy(np.array(baseline_df['patient_id']))\n",
    "num_patients = len(patients)\n",
    "def eval_vae(X_list, Z_list, var_param, encoder):\n",
    "    with torch.no_grad():\n",
    "        pat_ind_batch = [torch.arange(pat_ind[i],pat_ind[i+1]) for i in patients]\n",
    "        prior = Normal(torch.zeros(torch.Size([latent_dim])), torch.ones(torch.Size([latent_dim])))\n",
    "\n",
    "        test_data = torch.concatenate([torch.from_numpy(np.array(test_scores_df_encoded.loc[ind])).to(torch.float32) for ind in pat_ind_batch])\n",
    "\n",
    "        mu, log_sig = encoder.encode(test_data)\n",
    "\n",
    "        eps = prior.sample(torch.Size([log_sig.size(dim=0)])) \n",
    "        z = mu + log_sig.exp() * eps\n",
    "\n",
    "        z_list = [z[ind].flatten().to(torch.float32) for ind in pat_ind_batch]\n",
    "\n",
    "        Phi, sigma = var_param()\n",
    "        N = sum([len(Z_i) for Z_i in Z_list])\n",
    "\n",
    "        V_list = [Z_i @ Phi @ Z_i.t() + torch.eye(Z_i.size(0)) * sigma for Z_i in Z_list]\n",
    "        V_inv_list = [V_i.inverse() for V_i in V_list]\n",
    "        \n",
    "        Xt_V_inv_X = torch.stack([X_i.t() @ V_i_inv @ X_i for X_i, V_i_inv in zip(X_list, V_inv_list)]).sum(dim=0)\n",
    "        Xt_V_inv_y = torch.stack([X_i.t() @ V_i_inv @ y_i for X_i, V_i_inv, y_i in zip(X_list, V_inv_list, z_list)]).sum(dim=0)\n",
    "\n",
    "        EBLUE = Xt_V_inv_X.inverse() @ Xt_V_inv_y\n",
    "        EBLUP_list = [Phi @ Z_i.t() @ V_i_inv @ (y_i - X_i @ EBLUE) for X_i, Z_i, V_i_inv, y_i in zip(X_list, Z_list, V_inv_list, z_list)]\n",
    "\n",
    "        residual_list = [y_i - X_i @ EBLUE for y_i, X_i in zip(z_list, X_list)]\n",
    "        z_pred = torch.cat([X_i @ EBLUE + Z_i @ EBLUP_i for X_i, Z_i, EBLUP_i in zip(X_list, Z_list, EBLUP_list)]).reshape((-1, latent_dim))\n",
    "\n",
    "        log_det_V = torch.stack([V_i.det().clamp(min=1e-12).log() for V_i in V_list]).sum()\n",
    "        const = torch.log(torch.tensor(2.0 * torch.pi))\n",
    "        rt_V_inv_r = torch.stack([r_i.t() @ V_i_inv @ r_i for r_i, V_i_inv in zip(residual_list, V_inv_list)]).sum()\n",
    "\n",
    "        nML = 0.5 * (log_det_V + rt_V_inv_r + N * const) \n",
    "        return mu, z, z_pred, nML\n",
    "\n",
    "def likelihood_ratio(L_full, L_red):\n",
    "    return 2 * (L_full - L_red)\n",
    "\n",
    "\n",
    "fixed_effects_keys_full = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never', 'sex']\n",
    "random_effects_keys_full = ['intercept', 'since_medication', 'since_switch']\n",
    "# reduced model without fixed effect 'sex' \n",
    "fixed_effects_keys_red = ['family_affected', 'sco_surg', 'â‰¤3', 'onset_age', 'presym_diag', 'presymptomatic', 'stand_lost', 'stand_gained', 'stand_never']\n",
    "random_effects_keys_red = ['intercept', 'since_medication', 'since_switch']\n",
    "\n",
    "# get design matrix for the full model\n",
    "X_list_full, Z_list_full = get_design_matrix(df_effects, fixed_effects_keys_full, random_effects_keys_full, r=latent_dim)\n",
    "# get design matrix for the reduced model\n",
    "X_list_red, Z_list_red = get_design_matrix(df_effects, fixed_effects_keys_red, random_effects_keys_red, r=latent_dim)\n",
    "encoder_full, decoder_full, var_param_full = initialize(latent_dim) \n",
    "optimizer_vae_full = torch.optim.Adam([\n",
    "        {'params': var_param_full.parameters(), 'lr': 0.1},  \n",
    "        {'params': encoder_full.parameters(), 'lr': 0.01},  \n",
    "        {'params': decoder_full.parameters(), 'lr': 0.01},  \n",
    "])\n",
    "encoder_red, decoder_red, var_param_red = initialize(latent_dim) \n",
    "optimizer_vae_red  = torch.optim.Adam([\n",
    "        {'params': var_param_red.parameters(), 'lr': 0.1},  \n",
    "        {'params': encoder_red.parameters(), 'lr': 0.01},  \n",
    "        {'params': decoder_red.parameters(), 'lr': 0.01},  \n",
    "])\n",
    "# Lade die gespeicherten Daten\n",
    "#all_epochs_info = torch.load(r'C:\\Users\\yanni\\OneDrive\\Desktop\\BachelorArbeit2024\\Code\\trained_models\\complete_models_all_epochs.pth')\n",
    "lrt_results = []\n",
    "for i in range(0,38):\n",
    "    #epoch_to_load = f'epoch_{i}'\n",
    "    #loaded_data = all_epochs_info[epoch_to_load]\n",
    "    loaded_data = torch.load(fr'C:\\Users\\yanni\\OneDrive\\Desktop\\BachelorArbeit2024\\Code\\trained_models\\models_epoch_{i}.pth')\n",
    "\n",
    "    # Verwendung der geladenen Daten\n",
    "    encoder_full.load_state_dict(loaded_data['encoder_full_state_dict'])\n",
    "    decoder_full.load_state_dict(loaded_data['decoder_full_state_dict'])\n",
    "    var_param_full = loaded_data['var_param_full']\n",
    "    optimizer_vae_full.load_state_dict(loaded_data['optimizer_vae_full_state_dict'])\n",
    "    encoder_red.load_state_dict(loaded_data['encoder_red_state_dict'])\n",
    "    decoder_red.load_state_dict(loaded_data['decoder_red_state_dict'])\n",
    "    var_param_red = loaded_data['var_param_red']\n",
    "    optimizer_vae_red.load_state_dict(loaded_data['optimizer_vae_red_state_dict'])\n",
    "    X_list_full = loaded_data['X_list_full']\n",
    "    X_list_red = loaded_data['X_list_red']\n",
    "    Z_list_full = loaded_data['Z_list_full']\n",
    "    Z_list_red = loaded_data['Z_list_red']\n",
    "\n",
    "    nML_full = eval_vae(X_list_full,Z_list_full, var_param_full, encoder_full)[3]\n",
    "    nML_red = eval_vae(X_list_red, Z_list_red, var_param_red, encoder_red)[3]\n",
    "\n",
    "    lrt_results.append(likelihood_ratio(nML_full,nML_red))\n",
    "\n",
    "# plot the histogr<am of the LRT-statistic\n",
    "plt.hist(lrt_results, bins=50, density=True, alpha=0.6, color='g', label='Histogramm fÃ¼r 35 Simulationen')\n",
    "\n",
    "#x = np.linspace(-8000, 8000, 100)\n",
    "#plt.plot(x, chi2.pdf(x + 500, df=1), 'r-', lw=2, label='Chi-Quadrat-Verteilung (df=1)')\n",
    "\n",
    "# Beschriftungen hinzufÃ¼gen\n",
    "plt.title('Histogramm der Likelihood-Ratio-Teststatistiken')\n",
    "plt.xlabel('Teststatistik')\n",
    "plt.ylabel('HÃ¤ufigkeit')\n",
    "plt.legend()\n",
    "\n",
    "# Histogramm anzeigen\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
